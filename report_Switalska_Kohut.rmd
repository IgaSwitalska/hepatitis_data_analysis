---
title: "Hepatisis data analysis"
output:
  html_document:
    number_sections: yes
  pdf_document: default
---

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(visdat)
library(caret)
library(RANN)
library(corrplot)
library(plotly)
library(ggplot2)
library(resample)
library(DataExplorer)
library(imputeMulti)
library(mice)
library(rmarkdown)
library(repr)
library(tidyverse)
library(flextable)
library(imbalance)
library(cowplot)

library(ggpubr)
```

# Introduction. Data description

## Problem description 

We chose data: B) Medical diagnostics: Hepatitis Data Set. http://archive.ics.uci.edu/ml/datasets/Hepatitis

The main aim of our project is to analyze the dataset with clinical trial results of people with hepatitis and try to evaluate death risk. Hepatitis is a serious disease, inflammation of the liver from any cause, and it can lead to the death of a person. We want to find better diagnostic methods that should help to determine the risk of death due to hepatitis. 

## Data characteristics

First rows of our data frame.

```{r}
df <- read.table("hepatitis.data", sep = ",")
colnames(df) <- c("Class","Age","Sex","Steroid","Antivirals","Fatigue","Malaise","Anorexia","LiverBig","LiverFirm","SpleenPalpable","Spiders","Ascites","Varices","Bilirubin","AlkPhosphate","Sgot","Albumin","Protime","Histology") # nolint
```

```{r}
head(df)
```

# Preparing data

## Data types

We initially had the wrong data types, so we changed them to the correct ones.

```{r}
df[df == "?"] <- NA

df <- mutate_all(df, function(x) as.numeric(as.character(x)))
categorical <- c(1, 3:14, 20)
df[, categorical] <- replace(df[, categorical], df[, categorical] == 2, 0)
df[, categorical] <-  lapply(df[, categorical], as.factor)
```

## Missing values 

There are 5 features without missing values, 10 with a small amount, 4 with an allowed number, and 1 (Protime) with almost half of the missing values.

```{r}
plot_missing(df)
```

In all, we have 75 rows with missing values, which have a total of 167 missing values. So we can not remove these rows.

```{r}
sum(rowSums(is.na(df)) != 0)
sum(is.na(df))
```

We can see that the majority of variables have missing values only in several attributes, but there exist objects with several non-missing characteristics. 

In additional file we can also find md.pattern plot.

```{r}
vis_dat(df)
```

## Add missing values 

We used several methods to impute missing values: knnImpute, bagImpute, medianImpute, and function with different methods for different attributes (predictive mean matching for numeric data and logistic regression imputation for binary data where a factor is with 2 levels)

We need all numerical valuesto use the first 3 methods.

```{r}
df.new <- mutate_all(df, function(x) as.numeric(as.character(x)))
data_transform <- preProcess(df.new, method = "knnImpute")
data_transform2 <- preProcess(df.new, method = "bagImpute")
data_transform3 <- preProcess(df.new, method = "medianImpute")

df1 <- predict(data_transform, df.new)
df2 <- predict(data_transform2, df.new)
df3 <- predict(data_transform3, df.new)
```

Since knnImpute returns normalized data, we need to return to the initial form. Also, we have to round values for categorical variables.

```{r, echo=FALSE}
unstandarize <- function(data){
  for (i in 1:20) {
    column <- df.new[, i]
    if (i %in% c(15, 18)) {
      data[, i] <- data[, i] * sd(na.omit(column)) + mean(na.omit(column))
    } else {
      data[, i] <- round(data[, i] * sd(na.omit(column)) + mean(na.omit(column)))
    }
  }
  return(data)
}
```

```{r}
df1.standarized <- df1
df2[c(4:14, 16:17, 19)] <- round(df2[c(4:14, 16:17, 19)])
df3[c(4:14, 16:17, 19)] <- round(df3[c(4:14, 16:17, 19)])
df1 <- unstandarize(df1)
```

In this function we impute missing values 1 time (df4), and 5 times in order to get better results (df5)

```{r, results="hide"}
methods = c(" ", " ", " ", "logreg", " ", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "pmm", "pmm", "pmm", "pmm", "pmm", " ")
imp_single <- mice(df, m = 1, method = methods) # Impute missing values
df4 <- complete(imp_single)         # Store imputed data

imp_multi <- mice(df, method = methods)  # Impute missing values multiple times
df5 <- complete(imp_multi, 1)
```

Now, let's check if the distribution has changed.

```{r, echo=FALSE}
df$method <-  c(rep("omit", nrow(df)))
df1$method <- c(rep("knn", nrow(df1)))
df2$method <- c(rep("bag tree", nrow(df2)))
df3$method <- c(rep("median", nrow(df3)))
df4$method <- c(rep("mice single", nrow(df4)))
df5$method <- c(rep("mice", nrow(df5)))
df_all <- rbind(df, df1, df2, df3, df4, df5)
df_all$method <- as.factor(df_all$method)
```

```{r fig3, fig.height = 6, fig.width = 6, fig.align = 'center', warning=FALSE, message=FALSE}
ggplot(df_all, aes(x = LiverFirm, fill = method)) +
  scale_fill_brewer(palette = "Set2") +
  geom_bar(position = "dodge")
```

```{r fig2, fig.height = 7, fig.width = 7, fig.align = 'center', warning=FALSE, message=FALSE}
ggplot(df_all, aes(x = Protime, color = method)) +
  scale_fill_brewer(palette = "Set2") +
  geom_density(size = 1.1) +
  facet_grid(Class ~ .)
```

We can see, that the median method works badly for the attribute with lots of missing values (we have such a feature), so we can not use it. For the rest, results are similar, but we can also skip mice.single because mice is just an average value of 5 single using of this (so results must be better). So, we will compare three methods of imputing missing values: knn(df1), bag tree(df2), and mice(df5). 

```{r echo=FALSE}
df_all <- rbind(df, df1, df2, df5)
df_all$method <- as.factor(df_all$method)

df_all <- rbind(df, df1, df2, df3, df5)
df_all$method <- as.factor(df_all$method)
```

# EDA

## Summary
Below we present basic statistics for continuous variables for various types of missing data substitutions. The statics are not significantly different. However, density functions must also be taken into account. They will tell us more about distribution.

```{r echo=FALSE}
stats1 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Age)),2),
            stat_q1 = round(quantile(na.omit(Age), 0.25),2),
            stat_median = round(median(na.omit(Age)),2),
            stat_mean = round(mean(na.omit(Age)),2),
            stat_q3 = round(quantile(na.omit(Age), 0.75),2),
            stat_max = round(max(na.omit(Age)),2),
            stat_std = round(sd(na.omit(Age)),2))
stats1$col <- "Age"

stats2 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Bilirubin)),2),
            stat_q1 = round(quantile(na.omit(Bilirubin), 0.25),2),
            stat_median = round(median(na.omit(Bilirubin)),2),
            stat_mean = round(mean(na.omit(Bilirubin)),2),
            stat_q3 = round(quantile(na.omit(Bilirubin), 0.75),2),
            stat_max = round(max(na.omit(Bilirubin)),2),
            stat_std = round(sd(na.omit(Bilirubin)),2))
stats2$col <- "Bilirubin"

stats3 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(AlkPhosphate)),2),
            stat_q1 = round(quantile(na.omit(AlkPhosphate), 0.25),2),
            stat_median = round(median(na.omit(AlkPhosphate)),2),
            stat_mean = round(mean(na.omit(AlkPhosphate)),2),
            stat_q3 = round(quantile(na.omit(AlkPhosphate), 0.75),2),
            stat_max = round(max(na.omit(AlkPhosphate)),2),
            stat_std = round(sd(na.omit(AlkPhosphate)),2))
stats3$col <- "AlkPhosphate"

stats4 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Sgot)),2),
            stat_q1 = round(quantile(na.omit(Sgot), 0.25),2),
            stat_median = round(median(na.omit(Sgot)),2),
            stat_mean = round(mean(na.omit(Sgot)),2),
            stat_q3 = round(quantile(na.omit(Sgot), 0.75),2),
            stat_max = round(max(na.omit(Sgot)),2),
            stat_std = round(sd(na.omit(Sgot)),2))
stats4$col <- "Sgot"

stats5 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Albumin)),2),
            stat_q1 = round(quantile(na.omit(Albumin), 0.25),2),
            stat_median = round(median(na.omit(Albumin)),2),
            stat_mean = round(mean(na.omit(Albumin)),2),
            stat_q3 = round(quantile(na.omit(Albumin), 0.75),2),
            stat_max = round(max(na.omit(Albumin)),2),
            stat_std = round(sd(na.omit(Albumin)),2))
stats5$col <- "Albumin"

stats6 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Protime)),2),
            stat_q1 = round(quantile(na.omit(Protime), 0.25),2),
            stat_median = round(median(na.omit(Protime)),2),
            stat_mean = round(mean(na.omit(Protime)),2),
            stat_q3 = round(quantile(na.omit(Protime), 0.75),2),
            stat_max = round(max(na.omit(Protime)),2),
            stat_std = round(sd(na.omit(Protime)),2))
stats6$col <- "Protime"
all_stats <- bind_rows(stats1, stats2, stats3, stats4, stats5, stats6)
```

```{r echo=FALSE}
numeric_cols <- c("Age", "Bilirubin", "AlkPhosphate", "Sgot", "Albumin", "Protime")
stats <- c("min"," q1", "median", "mean", "q3", "max", "std")

#pivoting
all_stats1 = all_stats[c(1:15),] %>%
    pivot_longer(starts_with("stat")) %>%
    transmute(method, name=paste(col, "-", name), value) %>%
    pivot_wider()

all_stats2 = all_stats[c(16:30),] %>%
    pivot_longer(starts_with("stat")) %>%
    transmute(method, name=paste(col, "-", name), value) %>%
    pivot_wider()

#Creating the table
header_df1 = tibble(col_keys = names(all_stats1),
                   col = c("Method", rep(numeric_cols[1:3],each=7)),
                   stat = c("Method", rep(stats,3)))
header_df2 = tibble(col_keys = names(all_stats2),
                   col = c("Method", rep(numeric_cols[4:6],each=7)),
                   stat = c("Method", rep(stats,3)))

all_stats1 %>% 
    flextable() %>% 
    set_header_df(header_df1) %>%
    merge_v(part="header") %>% 
    merge_h(part="header") %>% 
    theme_box() %>% 
    align(align = "center", part = "header") %>% 
    autofit()

all_stats2 %>% 
    flextable() %>% 
    set_header_df(header_df2) %>%
    merge_v(part="header") %>% 
    merge_h(part="header") %>% 
    theme_box() %>% 
    align(align = "center", part = "header") %>% 
    autofit()
```

## Barplots

Insights:

  * We have imbalance problem in dependent variable;
  
  * We have imbalance problems in almost all features;
  
  * Also, there are several features where imbalance problems is not so large: Fatigue, Malaise, LiverFirm, Spiders;
  
  * We have several balanced attributes: Steroid, Histology.
  

```{r}
df1[, categorical] <-  lapply(df1[, categorical], as.factor)
plot_bar(df, by = "method", by_position = "dodge")
```

Insights:

  * We have more men than women in the dataset. Moreover no women died. This may suggest that men are more likely to die from hepatitis.
  
  * Taking antiviral medications reduces the likelihood of death.
  
  * Almost all of the people who died had symptoms of fatigue and increased liver.
  
  * Histology and spiders might be important (?)
  
```{r}
# df1[, categorical] <-  lapply(df1[, categorical], as.factor)
# 
# plot_bar(na.omit(df1[-21]), by = "Class", by_position = "dodge")
```

```{r}
plot_bar(df[-21], by = "Class", by_position = "dodge")
```

In the addition file, we can see bar plots for different methods of imputing missing values. The results are very similar.

## Histograms

We have only 6 continuous attributes: Age, Bilirubin, AlkPhosphate, Sgot, Albumin, Protime.

Insights:

  * All have unimodal distribution;
  
  * Albumin, Protime have a left-skewed distribution and Age, AlkPhosphate, Bilirubin, Sgot have a right-skewed distribution;
  
  * The most common value for Age is about 40, Albumin - 4, AlkPhosphate - 75, Bilirubin - 0.7, Protime - 100, and Sgot - 0;
  
  * Age looks like Gamma distribution, Albumin - Beta, AlkPhosphate - Log Normal, Bilirubin - Log Normal, Protime - Beta, and Sgot - Exponential.

```{r fig5, fig.height = 6, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
numeric_cols <- c("Bilirubin", "Albumin")
plot_histogram(df, geom_histogram_args = list("fill" = "#f8766d"))
```

Insights:

  * There is a big difference in distributions of different classes, because the number of values from class 1 is significantly less;
  
  * But, maybe, it will be difficult to separate objects from different classes, because they are overlapping. Only Albumin and Protime are a bit of difference. There are no values from class 1 with values of Albumin higher than 4.3 and a few values from class 1 with values of Protime higher than 50.

```{r fig6, fig.height = 10, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(df, aes(x = Age, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p2 <- ggplot(df, aes(x = Albumin, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p3 <- ggplot(df, aes(x = AlkPhosphate, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p4 <- ggplot(df, aes(x = Bilirubin, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p5 <- ggplot(df, aes(x = Protime, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p6 <- ggplot(df, aes(x = Sgot, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

ggarrange(p1, p2, p3, p4, p5, p6, ncol = 2, nrow = 3)
```
In the addition file, we can see density plots for different methods of imputing missing values. The results are very similar.

## Q-Q plot

We can see that q-q plot for Age, Albumin and Protime look no so bad, maybe this attributes have normal distribution. 

```{r fig7, fig.height = 10, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
plot_qq(df)
```

In the addition file, we can see Q=Q plots for different methods of imputing missing values. The results are very similar.

## Boxplots

Insights:

  * There are no outliers for Age, 3 for Protime, and a lot for other features.
  
  * It's better to use Albumin, Bilirubin, and Protime to classify because they have larger differences. 

```{r fig8, fig.height = 6, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
plot_boxplot(df, by = "Class")
```

In the addition file, we can see box plots for different methods of imputing missing values. The results are very similar.

## Correlation

To calculate correlation, we must use data without missing values, so we use data with imputing missing values using knn. In the addition file, we can see a correlation matrix for other methods of imputing missing values. The results are very similar.

We have a low correlation for all pairs of attributes. 

```{r fig1, fig.height = 6, fig.width = 6, fig.align = 'center'}
cor_matrix <- cor(df1[, sapply(df1, is.numeric)], method = "pearson")
corrplot(cor_matrix, tl.col = "black", addCoef.col = 1, number.cex = 0.9)
```

```{r}
# numeric_cols <- c("Age", "Bilirubin", "AlkPhosphate", "Sgot", "Albumin", "Protime")

p1 <- ggplot(df1, aes(x=Albumin, y=Bilirubin,color=Class)) + geom_point(size=3)
p2 <- ggplot(df1, aes(x=Albumin, y=AlkPhosphate,color=Class)) + geom_point(size=3)
p3 <- ggplot(df1, aes(x=Albumin, y=Protime,color=Class)) + geom_point(size=3)
p4 <- ggplot(df1, aes(x=Bilirubin, y=Protime,color=Class)) + geom_point(size=3)

plot_grid(p1, p2, p3, p4, labels = "AUTO")

```

# Clasification
```{r}
inTrain <- createDataPartition(y=df1$Class, times=1, p=0.75, list=FALSE)
train <- df1[inTrain,]
train.standarized <- df1.standarized[inTrain,]
test <- df1[-inTrain,]
```
## Solving class imbalance problem
```{r}
# oversampling
n_new <- sum(train$Class == 0) - sum(train$Class == 1)

newMWMOTE <- mwmote(train.standarized, numInstances = n_new)
newMWMOTE <- unstandarize(newMWMOTE)
train.balanced <- rbind(train[-21], newMWMOTE)
```

```{r}
plotComparison(train[-21], train.balanced, attrs = c("Bilirubin","Albumin", "Protime"))
```

## Linear regression

### Oversampling

```{r}
to_numeric <- function(data) {
  data[,categorical] <- lapply(data[, categorical], as.numeric)
  data[,categorical] <- data[,categorical] - 1
  return(data)
}
```

```{r}
train.balanced.num <- to_numeric(train.balanced)
train.num <- to_numeric(train)
test.num <- to_numeric(test)
```

```{r}
model.0 <- lm(Class~., data=train.balanced.num) 
summary(model.0)
```
Models of two variables based on boxplots.
```{r}
# two variables
model.1 <- lm(Class~Albumin+Bilirubin, data=train.balanced.num) 
model.2 <- lm(Class~Albumin+AlkPhosphate, data=train.balanced.num) 
model.3 <- lm(Class~Albumin+Protime, data=train.balanced.num) 
model.4 <- lm(Class~Bilirubin+Protime, data=train.balanced.num) 

a1 <- -model.1$coefficients['Albumin']/model.1$coefficients['Bilirubin']
b1 <- -(model.1$coefficients['(Intercept)']-0.5)/model.1$coefficients['Bilirubin']
a2 <- -model.2$coefficients['Albumin']/model.2$coefficients['AlkPhosphate']
b2 <- -(model.2$coefficients['(Intercept)']-0.5)/model.2$coefficients['AlkPhosphate']
a3 <- -model.3$coefficients['Albumin']/model.3$coefficients['Protime']
b3 <- -(model.3$coefficients['(Intercept)']-0.5)/model.3$coefficients['Protime']
a4 <- -model.4$coefficients['Bilirubin']/model.4$coefficients['Protime']
b4 <- -(model.4$coefficients['(Intercept)']-0.5)/model.4$coefficients['Protime']
# confusion.matrix1 <- table(round(pred1), test$Class)
# confusionMatrix(confusion.matrix1, positive = "1")
```

```{r}
# two variables
p1 <- ggplot(test, aes(x=Albumin, y=Bilirubin,color=Class)) + geom_point(size=3) + geom_abline(slope=a1,intercept=b1)
p2 <- ggplot(test, aes(x=Albumin, y=AlkPhosphate,color=Class)) + geom_point(size=3) + geom_abline(slope=a2,intercept=b2)
p3 <- ggplot(test, aes(x=Albumin, y=Protime,color=Class)) + geom_point(size=3) + geom_abline(slope=a3,intercept=b3)
p4 <- ggplot(test, aes(x=Bilirubin, y=Protime,color=Class)) + geom_point(size=3) + geom_abline(slope=a4,intercept=b4)

# title <- ggdraw() + draw_label("Comparison of linear regression classifiers", fontface='bold')
plot_grid(p1, p2, p3, p4, ncol=2,labels = "AUTO")
```
```{r}
# model.2 <- lm(Class~Sex+Anorexia+Spiders+Bilirubin, data=train.balanced.num) 
# pred2   <- predict(model.2, test.num)

# confusion.matrix2 <- table(round(pred2), test.num$Class)
# confusionMatrix(confusion.matrix2, positive = "1")
```
### Tresholding

```{r}
model.tr.0 <- lm(Class~., data=train.num[-21]) 
pred.tr.0 <- predict(model.tr.0, test.num[-21]) 
confusionMatrix(table(round(pred.tr.0), test.num$Class), positive = "1")
```

### Confusion matrix
### Cross-Validation (CV) 
#### k-fold cross-validation
#### leave-one-out
### Bootstrap-based methods
#### leave-one-out bootstrap,
#### .632 estimator,
#### .632+ estimator. 
###	ROC-curve


## K-nearest neighbors algorithm (K-NN)
### Method
### Confusion matrix
### Cross-Validation (CV) 
#### k-fold cross-validation
#### leave-one-out
### Bootstrap-based methods
#### leave-one-out bootstrap,
#### .632 estimator,
#### .632+ estimator. 
###	ROC-curve

## Linear discriminant analysis (LDA)
### Method
### Confusion matrix
### Cross-Validation (CV) 
#### k-fold cross-validation
#### leave-one-out
### Bootstrap-based methods
#### leave-one-out bootstrap,
#### .632 estimator,
#### .632+ estimator. 
###	ROC-curve

## Quadratic discriminant analysis (QDA)
### Method
### Confusion matrix
### Cross-Validation (CV) 
#### k-fold cross-validation
#### leave-one-out
### Bootstrap-based methods
#### leave-one-out bootstrap,
#### .632 estimator,
#### .632+ estimator. 
###	ROC-curve

## Logistic regression (LR)
### Method
### Confusion matrix
### Cross-Validation (CV) 
#### k-fold cross-validation
#### leave-one-out
### Bootstrap-based methods
#### leave-one-out bootstrap,
#### .632 estimator,
#### .632+ estimator. 
###	ROC-curve

## Random tree
### Method
### Confusion matrix
### Cross-Validation (CV) 
#### k-fold cross-validation
#### leave-one-out
### Bootstrap-based methods
#### leave-one-out bootstrap,
#### .632 estimator,
#### .632+ estimator. 
###	ROC-curve

## Random forest
### Method
### Confusion matrix
### Cross-Validation (CV) 
#### k-fold cross-validation
#### leave-one-out
### Bootstrap-based methods
#### leave-one-out bootstrap,
#### .632 estimator,
#### .632+ estimator. 
###	ROC-curve

