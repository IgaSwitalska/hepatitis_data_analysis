---
title: "Hepatisis data analysis"
output:
  html_document:
    number_sections: yes
  pdf_document: default
---

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(visdat)
library(caret)
library(RANN)
library(corrplot)
library(plotly)
library(ggplot2)
library(resample)
library(DataExplorer)
library(imputeMulti)
library(mice)
library(rmarkdown)
library(repr)
library(tidyverse)
library(flextable)
library(imbalance)
library(cowplot)
library(MASS)
library(GGally)
library(ipred)
```

# Introduction. Data description

## Problem description 

We chose data: B) Medical diagnostics: Hepatitis Data Set. http://archive.ics.uci.edu/ml/datasets/Hepatitis

The main aim of our project is to analyze the dataset with clinical trial results of people with hepatitis and try to evaluate death risk. Hepatitis is a serious disease, inflammation of the liver from any cause, and it can lead to the death of a person. We want to find better diagnostic methods that should help to determine the risk of death due to hepatitis. 

## Data characteristics

Our data set consist of 155 observations and 20 columns. The features are the following:

1. **Class** - a factor at two levels, which we want to predict (1 - patient dies, 2 - patient lives).

2. **Age** - age of the patients in years (from 20 to 80 years).

3. **Sex** - gender of patient, a factor at two levels coded by 1 (male) and 2 (female).

4. **Steroid** - steroid treatment, a factor at two levels coded by 1 (yes) and 2 (no).

5. **Antivirals** - antivirals medication, a factor at two levels 1 (yes) and 2 (no).

6. **Fatigue** - fatigue is a frequent and disabling symptom reported by patients with chronic hepatitis, a factor at two levels 1 (yes) and 2 (no).

7. **Malaise** - malaise one of the symptoms of hepatitis, a factor at two levels 1 (yes) and 2 (no).

8. **Anorexia** - anorexia, loss of appetite, a factor at two levels 1 (yes) and 2 (no).

9. **LiverBig** - the size of liver increased or fatty, a factor at two levels 1 (yes) and 2 (no).

10. **LiverFirm** - the liver is firm, a factor at two levels 1 (yes) and 2 (no).

11. **SpleenPalpable** - splenomegaly is an enlargement of the spleen, a factor at two levels 1 (yes) and 2 (no).

12. **Spiders** - enlarged blood vessels that resemble little spiders,a factor at two levels 1 (yes) and 2 (no).

13. **Ascites** - ascites is the presence of excess fluid in the peritoneal cavity, a factor at two levels 1 (yes) and 2 (no).

14. **Varices** - varicose veins are a medical condition in which superficial veins become enlarged and twisted, a factor at two levels 1 (yes) and 2 (no).

15. **Bilirubin** - bilirubin is a substance made when the body breaks down old red blood cells, continues variable measured in mg/dL (?) (range: 0.3 - 8).

16. **AlkPhosphate** - alkaline phosphatase is an enzyme made in liver cells and bile ducts, a discrete valued feature reveals level alkaline phosphatase measured in IU/L, where UI - international unit (range: 26 - 295). A 2013 research review showed that the normal range for a serum ALP level in healthy adults is 20 to 140 IU/L.

17. **Sgot** - a glutamic-oxaloacetic transaminase (SGOT) test measures the levels of the enzyme AST in the blood to assess liver health. A discrete valued feature measured in units per liter of serum (range: 14 - 648). If the results of your SGOT test are high, that means one of the organs or muscles containing the enzyme could be damaged. The normal range of an SGOT test is generally between 8 and 45 units per liter of serum.

18. **Albumin** - albumin is a family of globular proteins, the most common of which are the serum albumins. Low albumin levels can indicate a disorder of the liver or kidneys. Continues variable measured in mg/dL (?) (range: 2.1 - 6.4).

19. **Protime** - a discrete valued feature. How long it takes blood to form a clot in sec (range: 0 - 100). It shows how bad liver works.

20. **Histology** - histology is the branch of biology that studies the microscopic anatomy of biological tissues. A factor at two levels 1 (yes) and 2 (no).

```{r}
df <- read.table("hepatitis.data", sep = ",")
colnames(df) <- c("Class","Age","Sex","Steroid","Antivirals","Fatigue","Malaise","Anorexia","LiverBig","LiverFirm","SpleenPalpable","Spiders","Ascites","Varices","Bilirubin","AlkPhosphate","Sgot","Albumin","Protime","Histology") # nolint
```

Below we present the first rows of our data set. As can be seen, in addition to the fact that we will have to change the encoding from 1-2 to 0-1, we will also have to deal with missing values. In our dataset, they are marked as "?".

```{r}
head(df) %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

# Preparing data

## Data types

We initially had the wrong data types, so we changed them to the correct ones.

```{r}
df[df == "?"] <- NA

df <- mutate_all(df, function(x) as.numeric(as.character(x)))
categorical <- c(1, 3:14, 20)
df[, categorical] <- replace(df[, categorical], df[, categorical] == 2, 0)
df[, categorical] <-  lapply(df[, categorical], as.factor)
```

## Missing values 

There are 5 features without missing values, 10 with a small amount, 4 with an allowed number, and 1 (Protime) with almost half of the missing values.

```{r}
plot_missing(df)
```

In all, we have 75 rows with missing values, which have a total of 167 missing values. So we can not remove these rows.

```{r}
sum(rowSums(is.na(df)) != 0)
sum(is.na(df))
```

We can see that the majority of variables have missing values only in several attributes, but there exist objects with several non-missing characteristics. 

```{r}
vis_dat(df)
```

```{r, echo=FALSE}
# md.pattern(df)
```

## Add missing values 

We used several methods to impute missing values: knnImpute, bagImpute, medianImpute, and function with different methods for different attributes (predictive mean matching for numeric data and logistic regression imputation for binary data where a factor is with 2 levels)

We need all numerical valuesto use the first 3 methods.

```{r}
df.new <- mutate_all(df, function(x) as.numeric(as.character(x)))
data_transform <- preProcess(df.new, method = "knnImpute")
data_transform2 <- preProcess(df.new, method = "bagImpute")
data_transform3 <- preProcess(df.new, method = "medianImpute")

df1 <- predict(data_transform, df.new)
df2 <- predict(data_transform2, df.new)
df3 <- predict(data_transform3, df.new)
```

Since knnImpute returns normalized data, we need to return to the initial form. Also, we have to round values for intiger and categorical variables.

```{r, echo=FALSE}
unstandarize <- function(data){
  for (i in 1:20) {
    column <- df.new[, i]
    if (i %in% c(15, 18)) {
      data[, i] <- data[, i] * sd(na.omit(column)) + mean(na.omit(column))
    } else {
      data[, i] <- round(data[, i] * sd(na.omit(column)) + mean(na.omit(column)))
    }
  }
  return(data)
}
```

```{r}
df1.standarized <- df1
df2[c(4:14, 16:17, 19)] <- round(df2[c(4:14, 16:17, 19)])
df3[c(4:14, 16:17, 19)] <- round(df3[c(4:14, 16:17, 19)])
df1 <- unstandarize(df1)
```

In this function we impute missing values 1 time (df4), and 5 times in order to get better results (df5)

```{r, results="hide"}
methods = c(" ", " ", " ", "logreg", " ", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "pmm", "pmm", "pmm", "pmm", "pmm", " ")
imp_single <- mice(df, m = 1, method = methods) # Impute missing values
df4 <- complete(imp_single)         # Store imputed data

imp_multi <- mice(df, method = methods)  # Impute missing values multiple times
df5 <- complete(imp_multi, 1)
```

Now, let's check if the distribution has changed.

```{r, echo=FALSE}
df$method <-  c(rep("omit", nrow(df)))
df1$method <- c(rep("knn", nrow(df1)))
df2$method <- c(rep("bag tree", nrow(df2)))
df3$method <- c(rep("median", nrow(df3)))
df4$method <- c(rep("mice single", nrow(df4)))
df5$method <- c(rep("mice", nrow(df5)))
df_all <- rbind(df, df1, df2, df3, df4, df5)
df_all$method <- as.factor(df_all$method)
```

```{r fig3, fig.height = 6, fig.width = 6, fig.align = 'center', warning=FALSE, message=FALSE}
ggplot(na.omit(df_all), aes(x = LiverFirm, fill = method)) +
  scale_fill_brewer(palette = "Set2") +
  geom_bar(position = "dodge")
```

```{r fig2, fig.height = 7, fig.width = 7, fig.align = 'center', warning=FALSE, message=FALSE}
ggplot(na.omit(df_all), aes(x = Protime, color = method)) +
  scale_fill_brewer(palette = "Set2") +
  geom_density(size = 1.1) +
  facet_grid(Class ~ .)
```

We can see, that the median method works badly for the attribute with lots of missing values (we have such a feature), so we can not use it. For the rest, results are similar, but we can also skip mice.single because mice is just an average value of 5 single using of this (so results must be better). So, we will compare three methods of imputing missing values: knn, bag tree, and mice. 

```{r echo=FALSE}
df_all <- rbind(df, df1, df2, df5)
df_all$method <- as.factor(df_all$method)
```

# EDA

## Summary
Below we present basic statistics for continuous variables for various types of missing data substitutions. The statics are not significantly different. However, density functions must also be taken into account. They will tell us more about distribution. We have already presnted the density of "Protime" to justify the rejection of this method. Other distributions will be analyzed later in the report.

```{r echo=FALSE}
stats1 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Age)),2),
            stat_q1 = round(quantile(na.omit(Age), 0.25),2),
            stat_median = round(median(na.omit(Age)),2),
            stat_mean = round(mean(na.omit(Age)),2),
            stat_q3 = round(quantile(na.omit(Age), 0.75),2),
            stat_max = round(max(na.omit(Age)),2),
            stat_std = round(sd(na.omit(Age)),2))
stats1$col <- "Age"

stats2 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Bilirubin)),2),
            stat_q1 = round(quantile(na.omit(Bilirubin), 0.25),2),
            stat_median = round(median(na.omit(Bilirubin)),2),
            stat_mean = round(mean(na.omit(Bilirubin)),2),
            stat_q3 = round(quantile(na.omit(Bilirubin), 0.75),2),
            stat_max = round(max(na.omit(Bilirubin)),2),
            stat_std = round(sd(na.omit(Bilirubin)),2))
stats2$col <- "Bilirubin"

stats3 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(AlkPhosphate)),2),
            stat_q1 = round(quantile(na.omit(AlkPhosphate), 0.25),2),
            stat_median = round(median(na.omit(AlkPhosphate)),2),
            stat_mean = round(mean(na.omit(AlkPhosphate)),2),
            stat_q3 = round(quantile(na.omit(AlkPhosphate), 0.75),2),
            stat_max = round(max(na.omit(AlkPhosphate)),2),
            stat_std = round(sd(na.omit(AlkPhosphate)),2))
stats3$col <- "AlkPhosphate"

stats4 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Sgot)),2),
            stat_q1 = round(quantile(na.omit(Sgot), 0.25),2),
            stat_median = round(median(na.omit(Sgot)),2),
            stat_mean = round(mean(na.omit(Sgot)),2),
            stat_q3 = round(quantile(na.omit(Sgot), 0.75),2),
            stat_max = round(max(na.omit(Sgot)),2),
            stat_std = round(sd(na.omit(Sgot)),2))
stats4$col <- "Sgot"

stats5 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Albumin)),2),
            stat_q1 = round(quantile(na.omit(Albumin), 0.25),2),
            stat_median = round(median(na.omit(Albumin)),2),
            stat_mean = round(mean(na.omit(Albumin)),2),
            stat_q3 = round(quantile(na.omit(Albumin), 0.75),2),
            stat_max = round(max(na.omit(Albumin)),2),
            stat_std = round(sd(na.omit(Albumin)),2))
stats5$col <- "Albumin"

stats6 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Protime)),2),
            stat_q1 = round(quantile(na.omit(Protime), 0.25),2),
            stat_median = round(median(na.omit(Protime)),2),
            stat_mean = round(mean(na.omit(Protime)),2),
            stat_q3 = round(quantile(na.omit(Protime), 0.75),2),
            stat_max = round(max(na.omit(Protime)),2),
            stat_std = round(sd(na.omit(Protime)),2))
stats6$col <- "Protime"
all_stats <- bind_rows(stats1, stats2, stats3, stats4, stats5, stats6)
```

```{r echo=FALSE}
numeric_cols <- c("Age", "Bilirubin", "AlkPhosphate", "Sgot", "Albumin", "Protime")
stats <- c("min"," q1", "median", "mean", "q3", "max", "std")

#pivoting
all_stats1 = all_stats[c(1:12),] %>%
    pivot_longer(starts_with("stat")) %>%
    transmute(method, name=paste(col, "-", name), value) %>%
    pivot_wider()

all_stats2 = all_stats[c(13:24),] %>%
    pivot_longer(starts_with("stat")) %>%
    transmute(method, name=paste(col, "-", name), value) %>%
    pivot_wider()

#Creating the table
header_df1 = tibble(col_keys = names(all_stats1),
                   col = c("Method", rep(numeric_cols[1:3],each=7)),
                   stat = c("Method", rep(stats,3)))
header_df2 = tibble(col_keys = names(all_stats2),
                   col = c("Method", rep(numeric_cols[4:6],each=7)),
                   stat = c("Method", rep(stats,3)))

all_stats1 %>% 
    flextable() %>% 
    set_header_df(header_df1) %>%
    merge_v(part="header") %>% 
    merge_h(part="header") %>% 
    theme_box() %>% 
    align(align = "center", part = "header") %>% 
    autofit()

all_stats2 %>% 
    flextable() %>% 
    set_header_df(header_df2) %>%
    merge_v(part="header") %>% 
    merge_h(part="header") %>% 
    theme_box() %>% 
    align(align = "center", part = "header") %>% 
    autofit()
```

## Barplots

First, we will compare the distribution of binary variables for different missing values imputation methods. We can observe that only "Steroid" and  "Histology" attributes is balanced. Moreover, the distributions do not change across different imputation methods.

```{r}
plot_bar(na.omit(df_all), by = "method", by_position = "dodge")
```

Now we will try to assess the ability to separate objects from different classes by comparing the distribution of the target class for diffrent values of binary attributes. We will use only one data set (with missing values imputed using knn method) as the results for other data sets are similar.

Insights:

* We have more men than women in the dataset. Moreover no women died. This may suggest that men are more likely to die from hepatitis.
  
* Almost all of the people who died had symptoms of fatigue and did not take antiviral medications.

* Surprisingly, no enlarged liver was found in those who died.
  
* "Spiders" attribute might be important. The majority of people who died had spiders.
  
```{r}
df1[, categorical] <-  lapply(df1[, categorical], as.factor)

plot_bar(df1[-21], by = "Class", by_position = "dodge")
```

## Boxplots
Ability to separate objects from different classes: Protime, Albumin, Bilirubin.
```{r}
plot_boxplot(df1, by = "Class")
```

## Correlation

```{r echo=FALSE}
to_numeric <- function(data) {
  data[,categorical] <- lapply(data[, categorical], as.numeric)
  data[,categorical] <- data[,categorical] - 1
  return(data)
}
```

First we will analyze the Pearson correlation between continuous variables. According to the correlation matrix the Albumin is correlated to Bilirubin, AlkPhosphate and Protime. The Bilirubin and Protime attributes are also correlated. Furthermore, as can be seen on the scatter plots, combinations of Albumin and Protime with other attributes seem to be good at distinguishing between classes.

We can also observe two outliers (people with high level of Bilirubin).

```{r fig1, fig.height = 10, fig.width = 10, fig.align = 'center', echo=FALSE}
# cor_matrix <- cor(df1[, sapply(df1, is.numeric)], method = "pearson")
# corrplot(cor_matrix, tl.col = "black", addCoef.col = 1, number.cex = 0.9)
```

```{r fig_corr, fig.height = 10, fig.width = 10, fig.align = 'center'}
ggpairs(df1, columns = c(2,15:19), aes(color = Class, alpha = 0.5))
```

```{r echo=FALSE}
# numeric_cols <- c("Age", "Bilirubin", "AlkPhosphate", "Sgot", "Albumin", "Protime")

# p1 <- ggplot(df1, aes(x=Albumin, y=Bilirubin,color=Class)) + geom_point(size=1)
# p2 <- ggplot(df1, aes(x=Albumin, y=AlkPhosphate,color=Class)) + geom_point(size=1)
# p3 <- ggplot(df1, aes(x=Albumin, y=Protime,color=Class)) + geom_point(size=1)
# p4 <- ggplot(df1, aes(x=Bilirubin, y=Protime,color=Class)) + geom_point(size=1)

# plot_grid(p1, p2, p3, p4, labels = "AUTO")

```

When it comes to correlation between binary variables, "Fatigue" and "Malaise" are the most correlated. The target class, on the other hand, is most correlated with "Spiders", "Ascites", "Varices", "Histology" and "Malaise".

```{r fig_corr2, fig.height = 10, fig.width = 10, fig.align = 'center'}
cor_matrix <- cor(to_numeric(df1)[, categorical], method = "pearson")
corrplot(cor_matrix, tl.col = "black", addCoef.col = 1, number.cex = 0.9)
```


```{r echo=FALSE}
# plot_histogram(df)
# plot_bar(df)
# plot_qq(df)
# plot_boxplot(df, by = "Class")

## Charts: continuous quantitative data
# Typical questions:
  # 
  # - Uni- or multi-modal distribution?
  # - Symmetric or left- / right-skewed distribution?
  # - Mode / modal interval?
  # - Does the distribution resemble a normal, uniform distribution?

## Charts: discrete quantitative data
# Typical questions:
  #   
  #   - The most frequent value?
  #   - min / max?
  #   - Frequencies of consecutive values?

## Charts: qualitative data
  # Analysis within groups: Why do customers leave?
```


# Clasification
```{r}
set.seed(123) 
inTrain <- createDataPartition(y=df1$Class, times=1, p=0.75, list=FALSE)
train <- df1[inTrain,]
train.standarized <- df1.standarized[inTrain,]
test <- df1[-inTrain,]
test.standarized <- df1.standarized[-inTrain,]
```
## Solving class imbalance problem
```{r}
# oversampling
n_new <- sum(train$Class == 0) - sum(train$Class == 1)

newMWMOTE <- mwmote(train.standarized, numInstances = n_new)
train.balanced.std <- rbind(train.standarized[-21], newMWMOTE)
newMWMOTE <- unstandarize(newMWMOTE)
train.balanced <- rbind(train[-21], newMWMOTE)
```
```{r}
plotComparison(train[-21], train.balanced, attrs = c("Bilirubin","Albumin", "Protime"))
```

## Linear regression

### Oversampling

```{r}
train.balanced.num <- to_numeric(train.balanced)
train.num <- to_numeric(train)
test.num <- to_numeric(test)
```

```{r}
model.0 <- lm(Class~., data=train.balanced.num) 
summary(model.0)
```
Models of two variables based on boxplots.
```{r echo=FALSE}
slope <- function(model){
  -model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
  -(model$coefficients[1]-0.5)/model$coefficients[3]
}
lr_pred <- function(model,test,thr){
  pred <- predict(model, test)
  pred[pred > thr] = 1
  pred[pred < thr] = 0
  return(pred)
}
lda_pred <- function(model,test,thr){
  pred <- predict(model,test)$posterior[,2]
  pred[pred > thr] = 1
  pred[pred < thr] = 0
  return(pred)
}
metrices <- function(pred, real){
  confusion.matrix <- table(pred, real)
  a <- sum(diag(confusion.matrix))/sum(confusion.matrix)
  r <- recall(confusion.matrix, relevant = "1")
  p <- precision(confusion.matrix, relevant = "1")
  f <- 2 * p * r / (p + r)
  return(c(a,r,p,f))
}
```
```{r}
# two variables
model.1 <- lm(Class~Albumin+Bilirubin, data=train.balanced.num) 
model.2 <- lm(Class~Albumin+AlkPhosphate, data=train.balanced.num) 
model.3 <- lm(Class~Albumin+Sgot, data=train.balanced.num) 
model.4 <- lm(Class~Albumin+Protime, data=train.balanced.num) 
model.5 <- lm(Class~Protime+Age, data=train.balanced.num) 
model.6 <- lm(Class~Protime+Bilirubin, data=train.balanced.num) 
model.7 <- lm(Class~Protime+AlkPhosphate, data=train.balanced.num) 
model.8 <- lm(Class~Protime+Sgot, data=train.balanced.num) 
model.9 <- lm(Class~Albumin+Age, data=train.balanced.num) 

model.10 <- lm(Class~Sex+Steroid+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Protime, data=train.balanced.num) 
```

```{r fig_lr, fig.height = 10, fig.width = 10, fig.align = 'center', echo=FALSE}
# two variables
p1 <- ggplot(test, aes(x=Albumin, y=Bilirubin,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.1),intercept=intercept(model.1))
p2 <- ggplot(test, aes(x=Albumin, y=AlkPhosphate,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.2),intercept=intercept(model.2))
p3 <- ggplot(test, aes(x=Albumin, y=Sgot,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.3),intercept=intercept(model.3))
p4 <- ggplot(test, aes(x=Albumin, y=Protime,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.4),intercept=intercept(model.4))
p5 <- ggplot(test, aes(x=Protime, y=Age,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.5),intercept=intercept(model.5))
p6 <- ggplot(test, aes(x=Protime, y=Bilirubin,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.6),intercept=intercept(model.6))
p7 <- ggplot(test, aes(x=Protime, y=AlkPhosphate,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.7),intercept=intercept(model.7))
p8 <- ggplot(test, aes(x=Protime, y=Sgot,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.8),intercept=intercept(model.8))
p9 <- ggplot(test, aes(x=Albumin, y=Age,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.9),intercept=intercept(model.9))

# title <- ggdraw() + draw_label("Comparison of linear regression classifiers", fontface='bold')
plot_grid(p1, p2, p3, p4, p5, p6, p7, p8, p9, labels = "AUTO")
```
```{r, echo=FALSE}
All <- metrices(lr_pred(model.0, test.num, 0.5), test.num$Class)
Albumin.Bilirubin <- metrices(lr_pred(model.1, test.num, 0.5), test.num$Class)
Albumin.AlkPhosphate <- metrices(lr_pred(model.2, test.num, 0.5), test.num$Class)
Albumin.Sgot <- metrices(lr_pred(model.3, test.num, 0.5), test.num$Class)
Albumin.Protime <- metrices(lr_pred(model.4, test.num, 0.5), test.num$Class)
Protime.Age <- metrices(lr_pred(model.5, test.num, 0.5), test.num$Class)
Protime.Bilirubin <- metrices(lr_pred(model.6, test.num, 0.5), test.num$Class)
Protime.AlkPhosphate <- metrices(lr_pred(model.7, test.num, 0.5), test.num$Class)
Protime.Sgot <- metrices(lr_pred(model.8, test.num, 0.5), test.num$Class)
Albumin.Age <- metrices(lr_pred(model.9, test.num, 0.5), test.num$Class)

Important <- metrices(lr_pred(model.10, test.num, 0.5), test.num$Class)

lr_metrices1 <- data.frame(All, Important, Albumin.Age, Albumin.Bilirubin, Albumin.AlkPhosphate, Albumin.Sgot)
lr_metrices1 <- round(lr_metrices1,2)
lr_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lr_metrices1)

lr_metrices2 <- data.frame(Albumin.Protime, Protime.Age, Protime.Bilirubin, Protime.AlkPhosphate, Protime.Sgot)
lr_metrices2 <- round(lr_metrices2,2)
lr_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lr_metrices2)

lr_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lr_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```
## LDA 

### Thresholding

For the binary clasification, where classes distribution is balanced the LDA is equivalent to the linerar regression. Hence we will compare the linear discriminat model for the different threshlods calculated based on the cost matrix.

```{r}
model.0 <- lda(Class~., data=train.num[-21]) 
model.1 <- lda(Class~Albumin+Bilirubin, data=train.num[-21]) 
model.2 <- lda(Class~Albumin+AlkPhosphate, data=train.num[-21]) 
model.3 <- lda(Class~Albumin+Sgot, data=train.num[-21]) 
model.4 <- lda(Class~Albumin+Protime, data=train.num[-21]) 
model.5 <- lda(Class~Protime+Age, data=train.num[-21]) 
model.6 <- lda(Class~Protime+Bilirubin, data=train.num[-21]) 
model.7 <- lda(Class~Protime+AlkPhosphate, data=train.num[-21]) 
model.8 <- lda(Class~Protime+Sgot, data=train.num[-21]) 
model.9 <- lda(Class~Albumin+Age, data=train.num[-21]) 

model.10 <- lda(Class~Sex+Steroid+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Protime, data=train.num[-21]) 

```

```{r}
# model.0 <- lda(Class~., data=train.num[-21]) 
model.1 <- qda(Class~Albumin+Bilirubin, data=train.num[-21]) 
model.2 <- qda(Class~Albumin+AlkPhosphate, data=train.num[-21]) 
model.3 <- qda(Class~Albumin+Sgot, data=train.num[-21]) 
model.4 <- qda(Class~Albumin+Protime, data=train.num[-21]) 
model.5 <- qda(Class~Protime+Age, data=train.num[-21]) 
model.6 <- qda(Class~Protime+Bilirubin, data=train.num[-21]) 
model.7 <- qda(Class~Protime+AlkPhosphate, data=train.num[-21]) 
model.8 <- qda(Class~Protime+Sgot, data=train.num[-21]) 
model.9 <- qda(Class~Albumin+Age, data=train.num[-21]) 

model.10 <- lda(Class~Sex+Steroid+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Protime, data=train.num[-21]) 

```
```{r echo=FALSE}
p_thr = 0.5
test.num.copy <- test.num
test.num <- test.num[-21]

All <- metrices(lda_pred(model.0, test.num, p_thr), test.num$Class)
Albumin.Bilirubin <- metrices(lda_pred(model.1, test.num, p_thr), test.num$Class)
Albumin.AlkPhosphate <- metrices(lda_pred(model.2, test.num, p_thr), test.num$Class)
Albumin.Sgot <- metrices(lda_pred(model.3, test.num, p_thr), test.num$Class)
Albumin.Protime <- metrices(lda_pred(model.4, test.num, p_thr), test.num$Class)
Protime.Age <- metrices(lda_pred(model.5, test.num, p_thr), test.num$Class)
Protime.Bilirubin <- metrices(lda_pred(model.6, test.num, p_thr), test.num$Class)
Protime.AlkPhosphate <- metrices(lda_pred(model.7, test.num, p_thr), test.num$Class)
Protime.Sgot <- metrices(lda_pred(model.8, test.num, p_thr), test.num$Class)
Albumin.Age <- metrices(lda_pred(model.9, test.num, p_thr), test.num$Class)

Important <- metrices(lda_pred(model.10, test.num, p_thr), test.num$Class)

lda_metrices1 <- data.frame(All, Important, Albumin.Age, Albumin.Bilirubin, Albumin.AlkPhosphate, Albumin.Sgot)
lda_metrices1 <- round(lda_metrices1,2)
lda_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices1)

lda_metrices2 <- data.frame(Albumin.Protime, Protime.Age, Protime.Bilirubin, Protime.AlkPhosphate, Protime.Sgot)
lda_metrices2 <- round(lda_metrices2,2)
lda_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices2)

lda_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lda_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```
```{r echo=FALSE}
p_thr = 0.167
test.num.copy <- test.num
test.num <- test.num[-21]

All <- metrices(lda_pred(model.0, test.num, p_thr), test.num$Class)
Albumin.Bilirubin <- metrices(lda_pred(model.1, test.num, p_thr), test.num$Class)
Albumin.AlkPhosphate <- metrices(lda_pred(model.2, test.num, p_thr), test.num$Class)
Albumin.Sgot <- metrices(lda_pred(model.3, test.num, p_thr), test.num$Class)
Albumin.Protime <- metrices(lda_pred(model.4, test.num, p_thr), test.num$Class)
Protime.Age <- metrices(lda_pred(model.5, test.num, p_thr), test.num$Class)
Protime.Bilirubin <- metrices(lda_pred(model.6, test.num, p_thr), test.num$Class)
Protime.AlkPhosphate <- metrices(lda_pred(model.7, test.num, p_thr), test.num$Class)
Protime.Sgot <- metrices(lda_pred(model.8, test.num, p_thr), test.num$Class)
Albumin.Age <- metrices(lda_pred(model.9, test.num, p_thr), test.num$Class)

Important <- metrices(lda_pred(model.10, test.num, p_thr), test.num$Class)

lda_metrices1 <- data.frame(All, Important, Albumin.Age, Albumin.Bilirubin, Albumin.AlkPhosphate, Albumin.Sgot)
lda_metrices1 <- round(lda_metrices1,2)
lda_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices1)

lda_metrices2 <- data.frame(Albumin.Protime, Protime.Age, Protime.Bilirubin, Protime.AlkPhosphate, Protime.Sgot)
lda_metrices2 <- round(lda_metrices2,2)
lda_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices2)

lda_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lda_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

## KNN 

First we performed 5 times repeated 5-fold Cross Validation for a model that takes into account all variables. We chose the 5 most important features: Albumin, Ascites, Protime, Age, Bilirubin and check all their combinations. 

```{r echo=FALSE}
set.seed(123) 
k.grid <- data.frame(k=1:25)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)

knn.model.0 <- train(train.balanced.std[-1], train.balanced$Class, method="knn", tuneGrid =  k.grid, trControl=cvControl)

plot(varImp(knn.model.0, scale=FALSE))
```
```{r echo=FALSE}
lappend <- function (lst, ...){
lst <- c(lst, list(...))
  return(lst)
}
```
```{r}
comb <- list(c(18,13),c(18,19),c(18,2),c(18,15),c(13,19),c(13,2),c(13,15),c(19,2),c(19,15),c(2,15))

all_metrics <- list(c(metrices(predict(knn.model.0, newdata=test.standarized), test.num$Class), knn.model.0$bestTune[[1]]))
for (i in 1:10) {
  knn.model <- train(train.balanced.std[comb[[i]]], train.balanced$Class, method="knn", tuneGrid =  k.grid, trControl=cvControl)
  all_metrics <- lappend(all_metrics,c(metrices(predict(knn.model, newdata=test.standarized[comb[[i]]]), test.num$Class), knn.model$bestTune[[1]]))
}
```
```{r echo=FALSE}
knn_metrices <- data.frame(all_metrics)
colnames(knn_metrices) <- c("All","Albumin.Ascites", "Albumin.Protime", "Albumin.Age", "Albumin.Bilirubin", "Ascites.Protime",
"Ascites.Age","Ascites.Bilirubin","Protime.Age", "Protime.Bilirubin", "Age.Bilirubin")

knn_metrices <- round(knn_metrices,2)
knn_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure", "k"),knn_metrices)

knn_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```
```{r}
comb <- list(c(18,13,19),c(18,13,2),c(18,13,15),c(18,19,2),c(18,19,15),c(18,2,15),c(13,19,2),c(13,19,15),c(13,2,15),c(19,2,15))

all_metrics <- list(c(metrices(predict(knn.model.0, newdata=test.standarized), test.num$Class), knn.model.0$bestTune[[1]]))
for (i in 1:10) {
  knn.model <- train(train.balanced.std[comb[[i]]], train.balanced$Class, method="knn", tuneGrid =  k.grid, trControl=cvControl)
  all_metrics <- lappend(all_metrics,c(metrices(predict(knn.model, newdata=test.standarized[comb[[i]]]), test.num$Class), knn.model$bestTune[[1]]))
}
```
```{r echo=FALSE}
knn_metrices <- data.frame(all_metrics)
colnames(knn_metrices) <- c("All","Albumin.Ascites.Protime", "Albumin.Ascites.Age", "Albumin.Ascites.Bilirubin", "Albumin.Protime.Age", "Albumin.Protime.Bilirubin",
"Albumin.Age.Bilirubin","Ascites.Protime.Age","Ascites.Protime.Bilirubin", "Ascites.Age.Bilirubin", "Protime.Age.Bilirubin")

knn_metrices <- round(knn_metrices,2)
knn_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure", "k"),knn_metrices)

knn_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

As can be seen in the tables above the best results can be obtain for the following features: Protime, Bilirubin and Age. However, the best number of nearest neighbours selected during cross validation is 1, which may lead to overfitting. We will see if we can reduce the complexity of the algorithm. 

First we will plot the accuracy as a function of number of neighbours to see how it behaves. We can observe two things: the accuracy is not stable and drops significantly at the beginning.

```{r}
k.grid <- data.frame(k=1:25)

cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)

knn.model <- train(train.balanced.std[c(19,15,2)], train.balanced$Class, method="knn", tuneGrid =  k.grid, trControl=cvControl)

ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
```

We will repeat the cross validation excluding the possibilty of 1-NN algorithm.
```{r}
k.grid <- data.frame(k=2:25)

cvControl <- trainControl(method="boot",number=10)
# cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)
knn.model <- train(train.balanced.std[c(19,15,2)], train.balanced$Class, method="knn", tuneGrid =  k.grid, trControl=cvControl)

ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
```