---
title: "Hepatisis data analysis"
output:
  html_document:
    number_sections: yes
  pdf_document: default
---

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(visdat)
library(caret)
library(RANN)
library(corrplot)
library(plotly)
library(ggplot2)
library(resample)
library(DataExplorer)
library(imputeMulti)
library(mice)
library(rmarkdown)
library(repr)
library(tidyverse)
library(flextable)
library(imbalance)
library(cowplot)
library(ggpubr)
library(MASS)
library(GGally)
library(mlbench)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ipred)
library(ada)
library(class)
```

# Introduction. Data description

## Problem description 

We chose data: B) Medical diagnostics: Hepatitis Data Set. http://archive.ics.uci.edu/ml/datasets/Hepatitis

The main aim of our project is to analyze the dataset with clinical trial results of people with hepatitis and try to evaluate death risk. Hepatitis is a serious disease, inflammation of the liver from any cause, and it can lead to the death of a person. We want to find better diagnostic methods that should help to determine the risk of death due to hepatitis. 

## Data characteristics

Our data set consist of 155 observations and 20 columns. The features are the following:

1. **Class** - a factor at two levels, which we want to predict (1 - patient dies, 2 - patient lives).

2. **Age** - age of the patients in years (from 20 to 80 years).

3. **Sex** - gender of patient, a factor at two levels coded by 1 (male) and 2 (female).

4. **Steroid** - steroid treatment, a factor at two levels coded by 1 (yes) and 2 (no).

5. **Antivirals** - antivirals medication, a factor at two levels 1 (yes) and 2 (no).

6. **Fatigue** - fatigue is a frequent and disabling symptom reported by patients with chronic hepatitis, a factor at two levels 1 (yes) and 2 (no).

7. **Malaise** - malaise one of the symptoms of hepatitis, a factor at two levels 1 (yes) and 2 (no).

8. **Anorexia** - anorexia, loss of appetite, a factor at two levels 1 (yes) and 2 (no).

9. **LiverBig** - the size of liver increased or fatty, a factor at two levels 1 (yes) and 2 (no).

10. **LiverFirm** - the liver is firm, a factor at two levels 1 (yes) and 2 (no).

11. **SpleenPalpable** - splenomegaly is an enlargement of the spleen, a factor at two levels 1 (yes) and 2 (no).

12. **Spiders** - enlarged blood vessels that resemble little spiders,a factor at two levels 1 (yes) and 2 (no).

13. **Ascites** - ascites is the presence of excess fluid in the peritoneal cavity, a factor at two levels 1 (yes) and 2 (no).

14. **Varices** - varicose veins are a medical condition in which superficial veins become enlarged and twisted, a factor at two levels 1 (yes) and 2 (no).

15. **Bilirubin** - bilirubin is a substance made when the body breaks down old red blood cells.

16. **AlkPhosphate** - alkaline phosphatase is an enzyme made in liver cells and bile ducts, a discrete valued feature reveals level alkaline phosphatase measured in IU/L, where UI - international unit. A 2013 research review showed that the normal range for a serum ALP level in healthy adults is 20 to 140 IU/L.

17. **Sgot** - a glutamic-oxaloacetic transaminase (SGOT) test measures the levels of the enzyme AST in the blood to assess liver health. A discrete valued feature measured in units per liter of serum (range: 14 - 648). If the results of your SGOT test are high, that means one of the organs or muscles containing the enzyme could be damaged. The normal range of an SGOT test is generally between 8 and 45 units per liter of serum.

18. **Albumin** - albumin is a family of globular proteins, the most common of which are the serum albumins. Low albumin levels can indicate a disorder of the liver or kidneys.

19. **Protime** - a discrete valued feature. How long it takes blood to form a clot in sec. It shows how bad liver works.

20. **Histology** - histology is the branch of biology that studies the microscopic anatomy of biological tissues. A factor at two levels 1 (yes) and 2 (no).

```{r}
df <- read.table("hepatitis.data", sep = ",")
colnames(df) <- c("Class","Age","Sex","Steroid","Antivirals","Fatigue","Malaise","Anorexia","LiverBig","LiverFirm","SpleenPalpable","Spiders","Ascites","Varices","Bilirubin","AlkPhosphate","Sgot","Albumin","Protime","Histology") # nolint
```

Below we present the first rows of our data set. As can be seen, in addition to the fact that we will have to change the encoding from 1-2 to 0-1, we will also have to deal with missing values. In our dataset, they are marked as "?".

```{r}
head(df) %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

# Preparing data

## Data types

We initially had the wrong data types, so we changed them to the correct ones.

```{r}
df[df == "?"] <- NA

df <- mutate_all(df, function(x) as.numeric(as.character(x)))
categorical <- c(1, 3:14, 20)
df[, categorical] <- replace(df[, categorical], df[, categorical] == 2, 0)
df[, categorical] <-  lapply(df[, categorical], as.factor)
```

## Missing values 

There are 5 features without missing values, 10 with a small amount, 4 with an allowed number, and 1 (Protime) with almost half of the missing values.

```{r}
plot_missing(df)
```

In all, we have 75 rows with missing values, which have a total of 167 missing values. So we can not remove these rows.

```{r}
sum(rowSums(is.na(df)) != 0)
sum(is.na(df))
```

We can see that the majority of variables have missing values only in several attributes, but there exist objects with several non-missing characteristics. 

```{r}
vis_dat(df)
```

## Add missing values 

We used several methods to impute missing values: knnImpute (df1), bagImpute (df2), medianImpute (df3), and function with different methods for different attributes (predictive mean matching for numeric data and logistic regression imputation for binary data where a factor is with 2 levels)

We need all numerical values to use the first 3 methods.

```{r}
set.seed(123) 
df.new <- mutate_all(df, function(x) as.numeric(as.character(x)))
data_transform <- preProcess(df.new, method = "knnImpute")
data_transform2 <- preProcess(df.new, method = "bagImpute")
data_transform3 <- preProcess(df.new, method = "medianImpute")

df1 <- predict(data_transform, df.new)
df2 <- predict(data_transform2, df.new)
df3 <- predict(data_transform3, df.new)
```

Since knnImpute returns normalized data, we need to return to the initial form. Also, we have to round values for intiger and categorical variables.

```{r, echo=FALSE}
unstandarize <- function(data){
  for (i in 1:20) {
    column <- df.new[, i]
    if (i %in% c(15, 18)) {
      data[, i] <- data[, i] * sd(na.omit(column)) + mean(na.omit(column))
    } else {
      data[, i] <- round(data[, i] * sd(na.omit(column)) + mean(na.omit(column)))
    }
  }
  return(data)
}
```

```{r}
df1.standarized <- df1
df2[c(4:14, 16:17, 19)] <- round(df2[c(4:14, 16:17, 19)])
df3[c(4:14, 16:17, 19)] <- round(df3[c(4:14, 16:17, 19)])
df1 <- unstandarize(df1)
```

In this function we impute missing values 1 time (df4), and 5 times in order to get better results (df5)

```{r, results="hide"}
methods = c(" ", " ", " ", "logreg", " ", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "logreg", "pmm", "pmm", "pmm", "pmm", "pmm", " ")
imp_single <- mice(df, m = 1, method = methods) # Impute missing values
df4 <- complete(imp_single)         # Store imputed data

imp_multi <- mice(df, method = methods)  # Impute missing values multiple times
df5 <- complete(imp_multi, 1)
```

Now, let's check if the distribution has changed.

```{r, echo=FALSE}
df$method <-  c(rep("omit", nrow(df)))
df1$method <- c(rep("knn", nrow(df1)))
df2$method <- c(rep("bag tree", nrow(df2)))
df3$method <- c(rep("median", nrow(df3)))
df4$method <- c(rep("mice single", nrow(df4)))
df5$method <- c(rep("mice", nrow(df5)))
df_all <- rbind(df, df1, df2, df3, df4, df5)
df_all$method <- as.factor(df_all$method)
```

```{r fig3, fig.height = 6, fig.width = 6, fig.align = 'center', warning=FALSE, message=FALSE}
ggplot(na.omit(df_all), aes(x = LiverFirm, fill = method)) +
  scale_fill_brewer(palette = "Set2") +
  geom_bar(position = "dodge")
```

```{r fig2, fig.height = 7, fig.width = 7, fig.align = 'center', warning=FALSE, message=FALSE}
ggplot(na.omit(df_all), aes(x = Protime, color = method)) +
  scale_fill_brewer(palette = "Set2") +
  geom_density(size = 1.1) +
  facet_grid(Class ~ .)
```

The median method works badly for the attribute with many missing values (we have such a feature). We can see that the distributions of both LiverFirm and Protime have changed a lot compared to the original. So we can not use it. For the rest, results are similar, but we can also skip mice single because mice is just an average value of 5 single using of this (so results must be better). So, we will compare three methods of imputing missing values: knn(df1), bag tree(df2), and mice(df5). 

```{r echo=FALSE}
df_all <- rbind(df, df1, df2, df5)
df_all$method <- as.factor(df_all$method)
```

# EDA

## Data visualization in 2D

We visualize data in 2D using Principal component analysis (PCA).

In a 2D plot, it's difficult to split our data into 2 classes, because they are overlapping, but maybe in more space, the situation is better.

```{r echo=FALSE}
to_numeric1 <- function(data) {
  data <- mutate_all(data, function(x) as.numeric(as.character(x)))
  return(data)
}
```

```{r, fig.height = 6, fig.width = 6, fig.align = 'center', warning=FALSE, message=FALSE}
df1.num <- to_numeric1(df1)
df1.num.pca <- prcomp(df1.num[,2:20], center=T, scale=T)
pca <- data.frame(PC1 = df1.num.pca$x[,1], PC2 = df1.num.pca$x[,2], classes = as.factor(df1.num$Class))

ggplot(data = pca, aes(x = PC1, y = PC2)) +
    geom_point(aes(color = classes))
```

## Summary
Below we present basic statistics for continuous variables for various types of missing data substitutions. The statics are not significantly different. However, density functions must also be taken into account. They will tell us more about distribution. We have already presnted the density of "Protime" to justify the rejection of this method. Other distributions will be analyzed later in the report.

```{r echo=FALSE}
stats1 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Age)),2),
            stat_q1 = round(quantile(na.omit(Age), 0.25),2),
            stat_median = round(median(na.omit(Age)),2),
            stat_mean = round(mean(na.omit(Age)),2),
            stat_q3 = round(quantile(na.omit(Age), 0.75),2),
            stat_max = round(max(na.omit(Age)),2),
            stat_std = round(sd(na.omit(Age)),2))
stats1$col <- "Age"

stats2 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Bilirubin)),2),
            stat_q1 = round(quantile(na.omit(Bilirubin), 0.25),2),
            stat_median = round(median(na.omit(Bilirubin)),2),
            stat_mean = round(mean(na.omit(Bilirubin)),2),
            stat_q3 = round(quantile(na.omit(Bilirubin), 0.75),2),
            stat_max = round(max(na.omit(Bilirubin)),2),
            stat_std = round(sd(na.omit(Bilirubin)),2))
stats2$col <- "Bilirubin"

stats3 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(AlkPhosphate)),2),
            stat_q1 = round(quantile(na.omit(AlkPhosphate), 0.25),2),
            stat_median = round(median(na.omit(AlkPhosphate)),2),
            stat_mean = round(mean(na.omit(AlkPhosphate)),2),
            stat_q3 = round(quantile(na.omit(AlkPhosphate), 0.75),2),
            stat_max = round(max(na.omit(AlkPhosphate)),2),
            stat_std = round(sd(na.omit(AlkPhosphate)),2))
stats3$col <- "AlkPhosphate"

stats4 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Sgot)),2),
            stat_q1 = round(quantile(na.omit(Sgot), 0.25),2),
            stat_median = round(median(na.omit(Sgot)),2),
            stat_mean = round(mean(na.omit(Sgot)),2),
            stat_q3 = round(quantile(na.omit(Sgot), 0.75),2),
            stat_max = round(max(na.omit(Sgot)),2),
            stat_std = round(sd(na.omit(Sgot)),2))
stats4$col <- "Sgot"

stats5 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Albumin)),2),
            stat_q1 = round(quantile(na.omit(Albumin), 0.25),2),
            stat_median = round(median(na.omit(Albumin)),2),
            stat_mean = round(mean(na.omit(Albumin)),2),
            stat_q3 = round(quantile(na.omit(Albumin), 0.75),2),
            stat_max = round(max(na.omit(Albumin)),2),
            stat_std = round(sd(na.omit(Albumin)),2))
stats5$col <- "Albumin"

stats6 <- df_all %>%
  group_by(method) %>%
  summarize(stat_min = round(min(na.omit(Protime)),2),
            stat_q1 = round(quantile(na.omit(Protime), 0.25),2),
            stat_median = round(median(na.omit(Protime)),2),
            stat_mean = round(mean(na.omit(Protime)),2),
            stat_q3 = round(quantile(na.omit(Protime), 0.75),2),
            stat_max = round(max(na.omit(Protime)),2),
            stat_std = round(sd(na.omit(Protime)),2))
stats6$col <- "Protime"
all_stats <- bind_rows(stats1, stats2, stats3, stats4, stats5, stats6)
```

```{r echo=FALSE}
numeric_cols <- c("Age", "Bilirubin", "AlkPhosphate", "Sgot", "Albumin", "Protime")
stats <- c("min"," q1", "median", "mean", "q3", "max", "std")

#pivoting
all_stats1 = all_stats[c(1:12),] %>%
    pivot_longer(starts_with("stat")) %>%
    transmute(method, name=paste(col, "-", name), value) %>%
    pivot_wider()

all_stats2 = all_stats[c(13:24),] %>%
    pivot_longer(starts_with("stat")) %>%
    transmute(method, name=paste(col, "-", name), value) %>%
    pivot_wider()

#Creating the table
header_df1 = tibble(col_keys = names(all_stats1),
                   col = c("Method", rep(numeric_cols[1:3],each=7)),
                   stat = c("Method", rep(stats,3)))
header_df2 = tibble(col_keys = names(all_stats2),
                   col = c("Method", rep(numeric_cols[4:6],each=7)),
                   stat = c("Method", rep(stats,3)))

all_stats1 %>% 
    flextable() %>% 
    set_header_df(header_df1) %>%
    merge_v(part="header") %>% 
    merge_h(part="header") %>% 
    theme_box() %>% 
    align(align = "center", part = "header") %>% 
    autofit()

all_stats2 %>% 
    flextable() %>% 
    set_header_df(header_df2) %>%
    merge_v(part="header") %>% 
    merge_h(part="header") %>% 
    theme_box() %>% 
    align(align = "center", part = "header") %>% 
    autofit()
```

## Barplots

First, we will compare the distribution of binary variables. We will only show the results for the initial data frame, without imputed missing values, because the distributions do not change across different imputation methods. A comparison of the different imputation methods will be available in the supplementary materials.

Insights:

  * We have imbalance problem in dependent variable;
  
  * We have imbalance classes in almost all features;
  
  * Also, there are several features where imbalance is not so large: Fatigue, Malaise, LiverFirm, Spiders;
  
  * We have several balanced attributes: Steroid, Histology.
  
```{r}
plot_bar(df, by = "method", by_position = "dodge")
```

Now we will try to assess the ability to separate objects from different classes by comparing the distribution of the target class for diffrent values of binary attributes. We will use only one data set (with missing values imputed using knn method) as the results for other data sets are similar.

Insights:

* We have more men than women in the dataset. Moreover no women died. This may suggest that men are more likely to die from hepatitis.
  
* Almost all of the people who died had symptoms of fatigue and did not take antiviral medications.

* Surprisingly, no enlarged liver was found in those who died.
  
* "Spiders" attribute might be important. The majority of people who died had spiders.
  
```{r}
df1[, categorical] <-  lapply(df1[, categorical], as.factor)

plot_bar(df1[-21], by = "Class", by_position = "dodge")
```

## Histograms

We have only 6 continuous attributes: Age, Bilirubin, AlkPhosphate, Sgot, Albumin, Protime.

Insights:

  * All have unimodal distribution;
  
  * Albumin has a left-skewed distribution and AlkPhosphate, Bilirubin, Sgot have a right-skewed distribution. Also, Age and Protime have an almost symmetric distribution;
  
  * The most common value for Age is about 40, Albumin - 4, AlkPhosphate - 75, Bilirubin - 0.7, Protime - 100, and Sgot - 0;
  
  * Age looks like Gamma distribution, Albumin - Beta, AlkPhosphate - Log Normal, Bilirubin - Log Normal, Protime - Beta, and Sgot - Exponential.

```{r, fig.height = 6, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
plot_histogram(df, geom_histogram_args = list("fill" = "#f8766d"))
```

Insights:

  * There is a big difference in distributions of different classes, because the number of values from class 1 is significantly less;
  
  * But, maybe, it will be difficult to separate objects from different classes, because they are overlapping. Only Albumin and Protime are a bit of difference. There are no values from class 1 with values of Albumin higher than 4.3 and a few values from class 1 with values of Protime higher than 50.

```{r, fig.height = 10, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(df, aes(x = Age, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p2 <- ggplot(df, aes(x = Albumin, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p3 <- ggplot(df, aes(x = AlkPhosphate, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p4 <- ggplot(df, aes(x = Bilirubin, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p5 <- ggplot(df, aes(x = Protime, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

p6 <- ggplot(df, aes(x = Sgot, fill = Class)) +
   geom_histogram(color='#f8766d', alpha=0.6, position='identity')

ggarrange(p1, p2, p3, p4, p5, p6, ncol = 2, nrow = 3)
```
In the additional file, we can see density plots for different methods of imputing missing values. The results are very similar.

## Q-Q plot

We can see that q-q plot for Age, Albumin and Protime look no so bad, maybe this attributes have normal distribution. 

```{r, fig.height = 10, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
plot_qq(df)
```

In the additional file, we can see Q=Q plots for different methods of imputing missing values. The results are very similar.

## Boxplots

Insights:

  * There are no outliers for Age, 3 for Protime, and a lot for other features.
  
  * It's better to use Albumin, Bilirubin, and Protime to classify because they have larger differences. 

```{r, fig.height = 6, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
plot_boxplot(df, by = "Class")
```

In the additional file, we can see box plots for different methods of imputing missing values. The results are very similar.

## Correlation

First we will analyze the Pearson correlation between continuous variables. To calculate correlation, we must use data without missing values, so we use data with imputing missing values using knn. In the additional file, we can see a correlation matrix for other methods of imputing missing values. The results are very similar.

According to the correlation matrix the Albumin is correlated to Bilirubin, AlkPhosphate and Protime. The Bilirubin and Protime attributes are also correlated. Furthermore, as can be seen on the scatter plots, combinations of Albumin and Protime with other attributes seem to be good at distinguishing between classes.

We can also observe two outliers (people with high level of Bilirubin).

```{r fig_corr, fig.height = 10, fig.width = 10, fig.align = 'center'}
ggpairs(df1, columns = c(2,15:19), aes(color = Class, alpha = 0.5))
```

When it comes to correlation between binary variables, "Fatigue" and "Malaise" are the most correlated. The target class, on the other hand, is most correlated with "Spiders", "Ascites", "Varices", "Histology" and "Malaise".
```{r}
to_numeric <- function(data) {
  data[,categorical] <- lapply(data[, categorical], as.numeric)
  data[,categorical] <- data[,categorical] - 1
  return(data)
}
```

```{r fig_corr2, fig.height = 10, fig.width = 10, fig.align = 'center'}
cor_matrix <- cor(to_numeric(df1)[, categorical], method = "pearson")
corrplot(cor_matrix, tl.col = "black", addCoef.col = 1, number.cex = 0.9)
```

## Conclusions

* We need to impute missing values, and use for it 3 different methods;

* Results in EDA for all imputing data are similar;

* We have imbalance problem;

* Most of categorical attributes also have imbalanced classes;

* We have low correlation for all pair of features (< 0.41);

* Next fields seems to be important: Antivirals, Fatigue, LiverBig, Histology, Albumin,  Bilirubin, and Protime;

* Next variables seems useless: Steroid, Age, AlkPhosphate, and Sgot.


# Classification

We have the same values in variable Class in datasets for all imputing methods, so we can use the same inTrain.

```{r}
set.seed(123) 
inTrain <- createDataPartition(y=df1$Class, times=1, p=0.75, list=FALSE)

train1 <- df1[inTrain,-21]
test1 <- df1[-inTrain,-21]

train2 <- df2[inTrain,-21]
test2 <- df2[-inTrain,-21]

train3 <- df5[inTrain,-21]
test3 <- df5[-inTrain,-21]
```

## Solving class imbalance problem

We have the same values in variable Class in datasets for all imputing methods, so we can use the same n_new.

```{r}
# oversampling
n_new <- sum(train1$Class == 0) - sum(train1$Class == 1)
train.num1 <- to_numeric1(train1)
train.num2 <- to_numeric1(train2)
train.num3 <- to_numeric1(train3)

test.num1 <- to_numeric1(test1)
test.num2 <- to_numeric1(test2)
test.num3 <- to_numeric1(test3)

newMWMOTE1 <- mwmote(train.num1, numInstances = n_new)
train.balanced1 <- rbind(train.num1[-21], newMWMOTE1)

newMWMOTE2 <- mwmote(train.num2, numInstances = n_new)
train.balanced2 <- rbind(train.num2[-21], newMWMOTE2)

newMWMOTE3 <- mwmote(train.num3, numInstances = n_new)
train.balanced3 <- rbind(train.num3[-21], newMWMOTE3)
```

Let's check the proportion of the classes in train and test sets. We can see that they are similar. In a balanced set, classes are divided equally.

```{r}
prop.table(table(train1$Class))
prop.table(table(test1$Class))
prop.table(table(train.balanced1$Class))
```

Below we can check the comparison of values before and after oversampling for three selected variables.
```{r}
train.balanced1.2 <- train.balanced1
train.balanced1.2[, categorical] <-  lapply(train.balanced1.2[, categorical], as.factor)

plotComparison(train1, train.balanced1.2, attrs = c("Bilirubin","Albumin", "Protime"))
```

## Linear regression

Our first clasification is based linear regression. We fited the model, which took into account all the variables. As can be seen the importatnt variables are the following: Sex, Malaise, Anorexia, SpleenPalpable, Spiders, Bilurubin, Albumin and Protime.

```{r}
set.seed(123) 
model.0 <- lm(Class~., data=train.balanced1) 
summary(model.0)
```

```{r echo=FALSE}
slope <- function(model){
  -model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
  -(model$coefficients[1]-0.5)/model$coefficients[3]
}
lr_pred <- function(model,test,thr){
  pred <- predict(model, test)
  pred[pred > thr] = 1
  pred[pred < thr] = 0
  return(pred)
}
lda_pred <- function(model,test,thr){
  pred <- predict(model,test)$posterior[,2]
  pred[pred > thr] = 1
  pred[pred < thr] = 0
  return(pred)
}
metrices <- function(pred, real){
  confusion.matrix <- table(pred, real)
  a <- sum(diag(confusion.matrix))/sum(confusion.matrix)
  r <- recall(confusion.matrix, relevant = "1")
  p <- precision(confusion.matrix, relevant = "1")
  f <- 2 * p * r / (p + r)
  return(c(a,r,p,f))
}
```

We then decided to construct the model using important features. We also wanted to chceck if our initial insights were correct and fitted nine models using all combinations of Albumin and Protime with other attributes

```{r}
# two variables
model.1 <- lm(Class~Albumin+Bilirubin, data=train.balanced1) 
model.2 <- lm(Class~Albumin+AlkPhosphate, data=train.balanced1) 
model.3 <- lm(Class~Albumin+Sgot, data=train.balanced1) 
model.4 <- lm(Class~Albumin+Protime, data=train.balanced1) 
model.5 <- lm(Class~Protime+Age, data=train.balanced1) 
model.6 <- lm(Class~Protime+Bilirubin, data=train.balanced1) 
model.7 <- lm(Class~Protime+AlkPhosphate, data=train.balanced1) 
model.8 <- lm(Class~Protime+Sgot, data=train.balanced1) 
model.9 <- lm(Class~Albumin+Age, data=train.balanced1) 

model.10 <- lm(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.balanced1) 
```
The results for the test set are presented on the scatterplots below. Protime-Bilirubin and Protime-AlkPhosphate pairs seem best for distinguishing classes. We will also compare some metrics to better evaluate the performance of different models.
```{r fig_lr, fig.height = 10, fig.width = 10, fig.align = 'center', echo=FALSE}
# two variables
p1 <- ggplot(test1, aes(x=Albumin, y=Bilirubin,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.1),intercept=intercept(model.1))
p2 <- ggplot(test1, aes(x=Albumin, y=AlkPhosphate,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.2),intercept=intercept(model.2))
p3 <- ggplot(test1, aes(x=Albumin, y=Sgot,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.3),intercept=intercept(model.3))
p4 <- ggplot(test1, aes(x=Albumin, y=Protime,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.4),intercept=intercept(model.4))
p5 <- ggplot(test1, aes(x=Protime, y=Age,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.5),intercept=intercept(model.5))
p6 <- ggplot(test1, aes(x=Protime, y=Bilirubin,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.6),intercept=intercept(model.6))
p7 <- ggplot(test1, aes(x=Protime, y=AlkPhosphate,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.7),intercept=intercept(model.7))
p8 <- ggplot(test1, aes(x=Protime, y=Sgot,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.8),intercept=intercept(model.8))
p9 <- ggplot(test1, aes(x=Albumin, y=Age,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.9),intercept=intercept(model.9))

# title <- ggdraw() + draw_label("Comparison of linear regression classifiers", fontface='bold')
plot_grid(p1, p2, p3, p4, p5, p6, p7, p8, p9, labels = "AUTO")
```
As can be seen in the table below, we can get the best result if we take into account all important attributes. However, models based on Protime-Bilirubin and Protime-Age pairs also performed well.

```{r, echo=FALSE}
All <- metrices(lr_pred(model.0, test.num1, 0.5), test1$Class)
Albumin.Bilirubin <- metrices(lr_pred(model.1, test.num1, 0.5), test1$Class)
Albumin.AlkPhosphate <- metrices(lr_pred(model.2, test.num1, 0.5), test1$Class)
Albumin.Sgot <- metrices(lr_pred(model.3, test.num1, 0.5), test1$Class)
Albumin.Protime <- metrices(lr_pred(model.4, test.num1, 0.5), test1$Class)
Protime.Age <- metrices(lr_pred(model.5, test.num1, 0.5), test1$Class)
Protime.Bilirubin <- metrices(lr_pred(model.6, test.num1, 0.5), test1$Class)
Protime.AlkPhosphate <- metrices(lr_pred(model.7, test.num1, 0.5), test1$Class)
Protime.Sgot <- metrices(lr_pred(model.8, test.num1, 0.5), test1$Class)
Albumin.Age <- metrices(lr_pred(model.9, test.num1, 0.5), test1$Class)

Important <- metrices(lr_pred(model.10, test.num1, 0.5), test1$Class)

lr_metrices1 <- data.frame(All, Important, Albumin.Age, Albumin.Bilirubin, Albumin.AlkPhosphate, Albumin.Sgot)
lr_metrices1 <- round(lr_metrices1,2)
lr_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lr_metrices1)

lr_metrices2 <- data.frame(Albumin.Protime, Protime.Age, Protime.Bilirubin, Protime.AlkPhosphate, Protime.Sgot)
lr_metrices2 <- round(lr_metrices2,2)
lr_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lr_metrices2)

lr_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lr_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

The final step will be to compare the three best models for two other imputation methods. It can be easily seen that for the knn imputation method we obtain the best results. For all three models, the accuracy, recall, precision and F-measure are really high.

```{r}
model.6.1 <- lm(Class~Protime+Bilirubin, data=train.balanced1) 
model.6.2 <- lm(Class~Protime+Bilirubin, data=train.balanced2) 
model.6.3 <- lm(Class~Protime+Bilirubin, data=train.balanced3) 

model.5.1 <- lm(Class~Protime+Age, data=train.balanced1) 
model.5.2 <- lm(Class~Protime+Age, data=train.balanced2) 
model.5.3 <- lm(Class~Protime+Age, data=train.balanced3) 

model.10.1 <- lm(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.balanced1) 
model.10.2 <- lm(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.balanced2) 
model.10.3 <- lm(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.balanced3) 
```
```{r, echo=FALSE}
Protime.Age.1 <- metrices(lr_pred(model.5.1, test.num1, 0.5), test1$Class)
Protime.Age.2 <- metrices(lr_pred(model.5.2, test.num2, 0.5), test2$Class)
Protime.Age.3 <- metrices(lr_pred(model.5.3, test.num3, 0.5), test3$Class)

Protime.Bilirubin.1 <- metrices(lr_pred(model.6.1, test.num1, 0.5), test1$Class)
Protime.Bilirubin.2 <- metrices(lr_pred(model.6.2, test.num2, 0.5), test2$Class)
Protime.Bilirubin.3 <- metrices(lr_pred(model.6.3, test.num3, 0.5), test3$Class)

Important.1 <- metrices(lr_pred(model.10.1, test.num1, 0.5), test1$Class)
Important.2 <- metrices(lr_pred(model.10.2, test.num2, 0.5), test2$Class)
Important.3 <- metrices(lr_pred(model.10.3, test.num3, 0.5), test3$Class)

lr_metrices1 <- data.frame(Protime.Age.1, Protime.Age.2, Protime.Age.3)
lr_metrices1 <- round(lr_metrices1,2)
lr_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lr_metrices1)

lr_metrices2 <- data.frame(Protime.Bilirubin.1, Protime.Bilirubin.2, Protime.Bilirubin.3)
lr_metrices2 <- round(lr_metrices2,2)
lr_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lr_metrices2)

lr_metrices3 <- data.frame(Important.1, Important.2, Important.3)
lr_metrices3 <- round(lr_metrices3,2)
lr_metrices3 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lr_metrices3)

lr_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lr_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
    
lr_metrices3 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

To conclude, in the case of linear regression clasification. The best model is the one including: Sex, Malaise, Anorexia, SpleenPalpable, Spiders, Bilurubin, Albumin and Protime features and the best results can be obtained for the knn imputation method.

## Linear discriminant analysis (LDA)

For the binary clasification, where classes distribution is balanced the LDA is equivalent to the linerar regression. Hence this time we will use the inbalanced data set and compare LDA models for the different threshlods calculated based on the cost matrix. We will use the same features as before.

```{r}
model.0 <- lda(Class~., data=train.num1) 
model.1 <- lda(Class~Albumin+Bilirubin, data=train.num1) 
model.2 <- lda(Class~Albumin+AlkPhosphate, data=train.num1) 
model.3 <- lda(Class~Albumin+Sgot, data=train.num1) 
model.4 <- lda(Class~Albumin+Protime, data=train.num1) 
model.5 <- lda(Class~Protime+Age, data=train.num1) 
model.6 <- lda(Class~Protime+Bilirubin, data=train.num1) 
model.7 <- lda(Class~Protime+AlkPhosphate, data=train.num1) 
model.8 <- lda(Class~Protime+Sgot, data=train.num1) 
model.9 <- lda(Class~Albumin+Age, data=train.num1) 

model.10 <- lda(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.num1) 

```

First we will establish equal costs for classifying into incorrect classe. Hence the threshlod will be equal to 0.5. It can be observed that despite the high accuracy, recall for most of the cases is low.

```{r echo=FALSE}
p_thr = 0.5

All <- metrices(lda_pred(model.0, test.num1, p_thr), test1$Class)
Albumin.Bilirubin <- metrices(lda_pred(model.1, test.num1, p_thr), test1$Class)
Albumin.AlkPhosphate <- metrices(lda_pred(model.2, test.num1, p_thr), test1$Class)
Albumin.Sgot <- metrices(lda_pred(model.3, test.num1, p_thr), test1$Class)
Albumin.Protime <- metrices(lda_pred(model.4, test.num1, p_thr), test1$Class)
Protime.Age <- metrices(lda_pred(model.5, test.num1, p_thr), test1$Class)
Protime.Bilirubin <- metrices(lda_pred(model.6, test.num1, p_thr), test1$Class)
Protime.AlkPhosphate <- metrices(lda_pred(model.7, test.num1, p_thr), test1$Class)
Protime.Sgot <- metrices(lda_pred(model.8, test.num1, p_thr), test1$Class)
Albumin.Age <- metrices(lda_pred(model.9, test.num1, p_thr), test1$Class)

Important <- metrices(lda_pred(model.10, test.num1, p_thr), test1$Class)

lda_metrices1 <- data.frame(All, Important, Albumin.Age, Albumin.Bilirubin, Albumin.AlkPhosphate, Albumin.Sgot)
lda_metrices1 <- round(lda_metrices1,2)
lda_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices1)

lda_metrices2 <- data.frame(Albumin.Protime, Protime.Age, Protime.Bilirubin, Protime.AlkPhosphate, Protime.Sgot)
lda_metrices2 <- round(lda_metrices2,2)
lda_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices2)

lda_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lda_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

We will now adjust the cost of incorrect classification of dead people to 5, which will change our threshold to 0.167. As can be seen in the table below, we managed to increase recall (an important metric for our clasification) at the expense of precision. Just like in the case of linear regression the best models are Protime-Bilirubin and the one including all important features.

```{r echo=FALSE}
p_thr = 0.167

All <- metrices(lda_pred(model.0, test.num1, p_thr), test1$Class)
Albumin.Bilirubin <- metrices(lda_pred(model.1, test.num1, p_thr), test1$Class)
Albumin.AlkPhosphate <- metrices(lda_pred(model.2, test.num1, p_thr), test1$Class)
Albumin.Sgot <- metrices(lda_pred(model.3, test.num1, p_thr), test1$Class)
Albumin.Protime <- metrices(lda_pred(model.4, test.num1, p_thr), test1$Class)
Protime.Age <- metrices(lda_pred(model.5, test.num1, p_thr), test1$Class)
Protime.Bilirubin <- metrices(lda_pred(model.6, test.num1, p_thr), test1$Class)
Protime.AlkPhosphate <- metrices(lda_pred(model.7, test.num1, p_thr), test1$Class)
Protime.Sgot <- metrices(lda_pred(model.8, test.num1, p_thr), test1$Class)
Albumin.Age <- metrices(lda_pred(model.9, test.num1, p_thr), test1$Class)

Important <- metrices(lda_pred(model.10, test.num1, p_thr), test1$Class)

lda_metrices1 <- data.frame(All, Important, Albumin.Age, Albumin.Bilirubin, Albumin.AlkPhosphate, Albumin.Sgot)
lda_metrices1 <- round(lda_metrices1,2)
lda_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices1)

lda_metrices2 <- data.frame(Albumin.Protime, Protime.Age, Protime.Bilirubin, Protime.AlkPhosphate, Protime.Sgot)
lda_metrices2 <- round(lda_metrices2,2)
lda_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices2)

lda_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lda_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```
While comparing the results for different imputation methods similar conclusions may be drawn. Protime-Bilirubin for knn imputation method is the best model.

```{r echo=FALSE}
model.6.1 <- lda(Class~Protime+Bilirubin, data=train.num1) 
model.6.2 <- lda(Class~Protime+Bilirubin, data=train.num2) 
model.6.3 <- lda(Class~Protime+Bilirubin, data=train.num3) 

model.10.1 <- lda(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.num1) 
model.10.2 <- lda(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.num2) 
model.10.3 <- lda(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.num3) 
```
```{r, echo=FALSE}
p_thr = 0.167
Protime.Bilirubin.1 <- metrices(lda_pred(model.6.1, test.num1, p_thr), test1$Class)
Protime.Bilirubin.2 <- metrices(lda_pred(model.6.2, test.num2, p_thr), test2$Class)
Protime.Bilirubin.3 <- metrices(lda_pred(model.6.3, test.num3, p_thr), test3$Class)

Important.1 <- metrices(lda_pred(model.10.1, test.num1, p_thr), test1$Class)
Important.2 <- metrices(lda_pred(model.10.2, test.num2, p_thr), test2$Class)
Important.3 <- metrices(lda_pred(model.10.3, test.num3, p_thr), test3$Class)

lda_metrices1 <- data.frame(Protime.Bilirubin.1, Protime.Bilirubin.2, Protime.Bilirubin.3)
lda_metrices1 <- round(lda_metrices1,2)
lda_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices1)

lda_metrices2 <- data.frame(Important.1, Important.2, Important.3)
lda_metrices2 <- round(lda_metrices2,2)
lda_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices2)

lda_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
    
lda_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```
## Quadratic discriminant analysis (QDA)

For the QDA we will take into account only numeric variables.
```{r}
model.0 <- qda(Class~Age+Bilirubin+AlkPhosphate+Sgot+Albumin+Protime, data=train.balanced1)
model.1 <- qda(Class~Albumin+Bilirubin, data=train.balanced1) 
model.2 <- qda(Class~Albumin+AlkPhosphate, data=train.balanced1) 
model.3 <- qda(Class~Albumin+Sgot, data=train.balanced1) 
model.4 <- qda(Class~Albumin+Protime, data=train.balanced1) 
model.5 <- qda(Class~Protime+Age, data=train.balanced1) 
model.6 <- qda(Class~Protime+Bilirubin, data=train.balanced1) 
model.7 <- qda(Class~Protime+AlkPhosphate, data=train.balanced1) 
model.8 <- qda(Class~Protime+Sgot, data=train.balanced1) 
model.9 <- qda(Class~Albumin+Age, data=train.balanced1) 

# model.10 <- qda(Class~Sex+Malaise+Anorexia+SpleenPalpable+Spiders+Bilirubin+Albumin+Protime, data=train.balanced1) 

```

First we will establish equal costs for classifying into incorrect classe. Hence the threshlod will be equal to 0.5. It can be observed that despite the high accuracy, recall for most of the cases is low.

```{r echo=FALSE}
p_thr = 0.5

All <- metrices(lda_pred(model.0, test.num1, p_thr), test1$Class)
Albumin.Bilirubin <- metrices(lda_pred(model.1, test.num1, p_thr), test1$Class)
Albumin.AlkPhosphate <- metrices(lda_pred(model.2, test.num1, p_thr), test1$Class)
Albumin.Sgot <- metrices(lda_pred(model.3, test.num1, p_thr), test1$Class)
Albumin.Protime <- metrices(lda_pred(model.4, test.num1, p_thr), test1$Class)
Protime.Age <- metrices(lda_pred(model.5, test.num1, p_thr), test1$Class)
Protime.Bilirubin <- metrices(lda_pred(model.6, test.num1, p_thr), test1$Class)
Protime.AlkPhosphate <- metrices(lda_pred(model.7, test.num1, p_thr), test1$Class)
Protime.Sgot <- metrices(lda_pred(model.8, test.num1, p_thr), test1$Class)
Albumin.Age <- metrices(lda_pred(model.9, test.num1, p_thr), test1$Class)

# Important <- metrices(lda_pred(model.10, test.num1, p_thr), test1$Class)

lda_metrices1 <- data.frame(All, Albumin.Age, Albumin.Bilirubin, Albumin.AlkPhosphate, Albumin.Sgot)
lda_metrices1 <- round(lda_metrices1,2)
lda_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices1)

lda_metrices2 <- data.frame(Albumin.Protime, Protime.Age, Protime.Bilirubin, Protime.AlkPhosphate, Protime.Sgot)
lda_metrices2 <- round(lda_metrices2,2)
lda_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices2)

lda_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

lda_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```
## Logistic regression (LR)

We need numeric data. In the beginning we use all attributes for different imputing methods.

```{r, warning=FALSE, message=FALSE}
model.logit1 <-  glm(Class~.-Class, data=train.balanced1, family=binomial(link="logit"))
model.logit2 <-  glm(Class~.-Class, data=train.balanced2, family=binomial(link="logit"))
model.logit3 <-  glm(Class~.-Class, data=train.balanced3, family=binomial(link="logit"))
```

Now, we build the logistic regression model without variables that are unnecessary from the summary and useless from EDA.

```{r, warning=FALSE, message=FALSE}
model.logit4 <-  glm(Class~.-Class-Age-Anorexia-Bilirubin, data=train.balanced1, family=binomial(link="logit"))
model.logit5 <-  glm(Class~.-Class-Age-Anorexia-Bilirubin, data=train.balanced2, family=binomial(link="logit"))
model.logit6 <-  glm(Class~.-Class-Age-Anorexia-Bilirubin, data=train.balanced3, family=binomial(link="logit"))

model.logit7 <-  glm(Class~.-Class-Age-Steroid-AlkPhosphate-Sgot, data=train.balanced1, family=binomial(link="logit"))
model.logit8 <-  glm(Class~.-Class-Age-Steroid-AlkPhosphate-Sgot, data=train.balanced2, family=binomial(link="logit"))
model.logit9 <-  glm(Class~.-Class-Age-Steroid-AlkPhosphate-Sgot, data=train.balanced3, family=binomial(link="logit"))
```

In the additional file, we can see summaries for all logistic regression models. There is a difference between statistics, but not so big.

Now, let's predict the posterior probability i.e. Pr(0|x). Plot results where color - class.
Since we have the same splits, we can use the same n, p, colors.

There are false predictions in all models, but not so many.

```{r}
pred.prob1 <- predict(model.logit1, test.num1, type = "response")
pred.prob2 <- predict(model.logit2, test.num2, type = "response")
pred.prob3 <- predict(model.logit3, test.num3, type = "response")

pred.prob4 <- predict(model.logit4, test.num1, type = "response")
pred.prob5 <- predict(model.logit5, test.num2, type = "response")
pred.prob6 <- predict(model.logit6, test.num3, type = "response")

pred.prob7 <- predict(model.logit7, test.num1, type = "response")
pred.prob8 <- predict(model.logit8, test.num2, type = "response")
pred.prob9 <- predict(model.logit9, test.num3, type = "response")
```

```{r, echo=FALSE}
n <- nrow(test.num1)
classes <- as.factor(test.num1$Class)

pred.prob.plot1 <- data.frame(x = 1:n, probability = pred.prob1, classes = classes)
pred.prob.plot2 <- data.frame(x = 1:n, probability = pred.prob2, classes = classes)
pred.prob.plot3 <- data.frame(x = 1:n, probability = pred.prob3, classes = classes)

pred.prob.plot4 <- data.frame(x = 1:n, probability = pred.prob4, classes = classes)
pred.prob.plot5 <- data.frame(x = 1:n, probability = pred.prob5, classes = classes)
pred.prob.plot6 <- data.frame(x = 1:n, probability = pred.prob6, classes = classes)

pred.prob.plot7 <- data.frame(x = 1:n, probability = pred.prob7, classes = classes)
pred.prob.plot8 <- data.frame(x = 1:n, probability = pred.prob8, classes = classes)
pred.prob.plot9 <- data.frame(x = 1:n, probability = pred.prob9, classes = classes)
```

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(data = pred.prob.plot1, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p2 <- ggplot(data = pred.prob.plot2, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p3 <- ggplot(data = pred.prob.plot3, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p4 <- ggplot(data = pred.prob.plot4, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p5 <- ggplot(data = pred.prob.plot5, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p6 <- ggplot(data = pred.prob.plot6, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p7 <- ggplot(data = pred.prob.plot7, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p8 <- ggplot(data = pred.prob.plot8, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p9 <- ggplot(data = pred.prob.plot9, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, ncol = 3, nrow = 3)
```

Let's also plot histograms of predicted probabilities.

Most probabilities for all models are about 0 or 1, what we need.

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(pred.prob.plot1, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p2 <- ggplot(pred.prob.plot2, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p3 <- ggplot(pred.prob.plot3, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p4 <- ggplot(pred.prob.plot4, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p5 <- ggplot(pred.prob.plot5, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p6 <- ggplot(pred.prob.plot6, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p7 <- ggplot(pred.prob.plot7, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p8 <- ggplot(pred.prob.plot8, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p9 <- ggplot(pred.prob.plot9, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, ncol = 3, nrow = 3)
```

Now, let's convert probabilities to class labels for a given cutoff

```{r, echo=FALSE}
prob.to.labels <- function(probs, cutoff){
  classes <- rep("0",length(probs))
  classes[probs>cutoff] <- "1"
  return(as.factor(classes))
}
```

```{r}
pred.labels1 <- prob.to.labels(probs=pred.prob1, cutoff=0.5)
pred.labels2 <- prob.to.labels(probs=pred.prob2, cutoff=0.5)
pred.labels3 <- prob.to.labels(probs=pred.prob3, cutoff=0.5)

pred.labels4 <- prob.to.labels(probs=pred.prob4, cutoff=0.5)
pred.labels5 <- prob.to.labels(probs=pred.prob5, cutoff=0.5)
pred.labels6 <- prob.to.labels(probs=pred.prob6, cutoff=0.5)

pred.labels7 <- prob.to.labels(probs=pred.prob7, cutoff=0.5)
pred.labels8 <- prob.to.labels(probs=pred.prob8, cutoff=0.5)
pred.labels9 <- prob.to.labels(probs=pred.prob9, cutoff=0.5)

real.labels <- test.num1$Class
```


```{r}
logit1 <- metrices(pred.labels1, real.labels)
logit2 <- metrices(pred.labels2, real.labels)
logit3 <- metrices(pred.labels3, real.labels)

logit4 <- metrices(pred.labels4, real.labels)
logit5 <- metrices(pred.labels5, real.labels)
logit6 <- metrices(pred.labels6, real.labels)

logit7 <- metrices(pred.labels7, real.labels)
logit8 <- metrices(pred.labels8, real.labels)
logit9 <- metrices(pred.labels9, real.labels)

logit_metrices <- data.frame(logit1, logit2, logit3, logit4, logit5, logit6, logit7, logit8, logit9)
logit_metrices <- round(logit_metrices,2)
logit_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"), logit_metrices)

logit_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

## KNN 

First we performed 5 times repeated 5-fold Cross Validation for a model that takes into account all variables. We chose the 5 most important features: Albumin, Ascites, Protime, Malaise, Bilirubin and check all their combinations. 

```{r echo=FALSE}
set.seed(123) 
k.grid <- data.frame(k=1:25)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)

dataTransform <- preProcess(train.balanced1, method=c("center", "scale"))
train.balanced1.std <- predict(dataTransform, train.balanced1)
# train.balanced1.std$Class <- as.factor(train.balanced1$Class)

dataTransform <- preProcess(test.num1, method=c("center", "scale"))
test.num1.std <- predict(dataTransform, test.num1)
# test.num1.std$Class <- as.factor(test.num1$Class)

knn.model.0 <- train(train.balanced1.std[-1], as.factor(train.balanced1$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)

plot(varImp(knn.model.0, scale=FALSE))
```
```{r echo=FALSE}
lappend <- function (lst, ...){
lst <- c(lst, list(...))
  return(lst)
}
```
```{r}
set.seed(123) 
comb <- list(c(18,13),c(18,19),c(18,7),c(18,15),c(13,19),c(13,7),c(13,15),c(19,7),c(19,15),c(7,15))

all_metrics <- list(c(metrices(predict(knn.model.0, newdata=test.num1.std[-1]), test1$Class), knn.model.0$bestTune[[1]]))
for (i in 1:10) {
  knn.model <- train(train.balanced1.std[comb[[i]]], as.factor(train.balanced1$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
  all_metrics <- lappend(all_metrics,c(metrices(predict(knn.model, newdata=test.num1.std[comb[[i]]]), test1$Class), knn.model$bestTune[[1]]))
}
```
```{r echo=FALSE}
knn_metrices <- data.frame(all_metrics)
colnames(knn_metrices) <- c("All","Albumin.Ascites", "Albumin.Protime", "Albumin.Malaise", "Albumin.Bilirubin", "Ascites.Protime",
"Ascites.Malaise","Ascites.Bilirubin","Protime.Malaise", "Protime.Bilirubin", "Malaise.Bilirubin")

knn_metrices <- round(knn_metrices,2)
knn_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure", "k"),knn_metrices)

knn_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()

```
```{r}
set.seed(123) 
comb <- list(c(18,13,19),c(18,13,7),c(18,13,15),c(18,19,7),c(18,19,15),c(18,7,15),c(13,19,7),c(13,19,15),c(13,7,15),c(19,7,15))

all_metrics <- list(c(metrices(predict(knn.model.0, newdata=test.num1.std[-1]), test1$Class), knn.model.0$bestTune[[1]]))
for (i in 1:10) {
  knn.model <- train(train.balanced1.std[comb[[i]]], as.factor(train.balanced1$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
  all_metrics <- lappend(all_metrics,c(metrices(predict(knn.model, newdata=test.num1.std[comb[[i]]]), test1$Class), knn.model$bestTune[[1]]))
}
```
```{r echo=FALSE}
knn_metrices <- data.frame(all_metrics)
colnames(knn_metrices) <- c("All","Albumin.Ascites.Protime", "Albumin.Ascites.Malaise", "Albumin.Ascites.Bilirubin", "Albumin.Protime.Malaise", "Albumin.Protime.Bilirubin",
"Albumin.Malaise.Bilirubin","Ascites.Protime.Malaise","Ascites.Protime.Bilirubin", "Ascites.Malaise.Bilirubin", "Protime.Malaise.Bilirubin")

knn_metrices <- round(knn_metrices,2)
knn_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure", "k"),knn_metrices)

knn_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

As can be seen in the tables above the best results can be obtain for the following features: Protime-Bilirubin and Ascites-Protime-Malaise. We will plot the accuracy as a function of number of neighbours to see how it behaves. For both cases the accuracy first increases and then started to decrease. Hence we will consider the value selected using cross validation to be optimal.

```{r echo=FALSE}
set.seed(123) 
k.grid <- data.frame(k=1:25)

cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)

knn.model <- train(train.balanced1.std[c(19,15)], as.factor(train.balanced1$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)

ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
```

```{r echo=FALSE}
set.seed(123) 
k.grid <- data.frame(k=1:25)

cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)

knn.model <- train(train.balanced1.std[c(7,13,19)], as.factor(train.balanced1$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)

ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
```

We will finally compare different imputation methods. 

```{r echo=FALSE}
dataTransform <- preProcess(train.balanced2, method=c("center", "scale"))
train.balanced2.std <- predict(dataTransform, train.balanced1)
train.balanced2.std$Class <- as.factor(train.balanced2$Class)

dataTransform <- preProcess(test.num2, method=c("center", "scale"))
test.num2.std <- predict(dataTransform, test.num2)
test.num2.std$Class <- as.factor(test.num2$Class)

dataTransform <- preProcess(train.balanced3, method=c("center", "scale"))
train.balanced3.std <- predict(dataTransform, train.balanced3)
train.balanced3.std$Class <- as.factor(train.balanced3$Class)

dataTransform <- preProcess(test.num3, method=c("center", "scale"))
test.num3.std <- predict(dataTransform, test.num3)
test.num3.std$Class <- as.factor(test.num3$Class)
```
```{r}
set.seed(123) 
train.balanced1.std$Class <- as.factor(train.balanced1$Class)
test.num1.std$Class <- as.factor(test.num1$Class)

model.1.1 <- knn(train.balanced1.std[c(19,15)], test.num1.std[c(19,15)], train.balanced1.std$Class, k=18)
model.1.2 <- knn(train.balanced2.std[c(19,15)], test.num2.std[c(19,15)], train.balanced2.std$Class, k=18)
model.1.3 <- knn(train.balanced3.std[c(19,15)], test.num3.std[c(19,15)], train.balanced3.std$Class, k=18)

model.2.1 <- knn(train.balanced1.std[c(7,13,19)], test.num1.std[c(7,13,19)], train.balanced1.std$Class, k=7)
model.2.2 <- knn(train.balanced2.std[c(7,13,19)], test.num2.std[c(7,13,19)], train.balanced2.std$Class, k=7)
model.2.3 <- knn(train.balanced3.std[c(7,13,19)], test.num3.std[c(7,13,19)], train.balanced3.std$Class, k=7)
```
```{r, echo=FALSE}
Protime.Bilirubin.1 <- metrices(model.1.1, test1$Class)
Protime.Bilirubin.2 <- metrices(model.1.2, test2$Class)
Protime.Bilirubin.3 <- metrices(model.1.3, test3$Class)

Ascites.Protime.Malaise.1 <- metrices(model.2.1, test1$Class)
Ascites.Protime.Malaise.2 <- metrices(model.2.2, test2$Class)
Ascites.Protime.Malaise.3 <- metrices(model.2.3, test3$Class)

lda_metrices1 <- data.frame(Protime.Bilirubin.1, Protime.Bilirubin.2, Protime.Bilirubin.3)
lda_metrices1 <- round(lda_metrices1,2)
lda_metrices1 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metrices1)

lda_metrices2 <- data.frame(Ascites.Protime.Malaise.1, Ascites.Protime.Malaise.2, Ascites.Protime.Malaise.3)
lda_metrices2 <- round(lda_metrices2,2)
lda_metrices2 <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"),lda_metricesa)

lda_metrices1 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
    
lda_metrices2 %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

## Random tree

We can use both numeric and categorical data. Let's specify model formula. We use all attributes and also model without variables that are useless from EDA.

```{r}
mod1 <- Class ~ . - Class
mod2 <- Class ~ . - Class - Age - Steroid - AlkPhosphate - Sgot
```

Building full trees for different models and different datasets (for different imputation methods and initial with missing values).

```{r, echo=FALSE}
to_categorical <- function(data){
  data[, categorical] <-  lapply(data[, categorical], as.factor)
  return(data)
}

train1 <- to_categorical(train1)
train2 <- to_categorical(train2)
train3 <- to_categorical(train3)

test1 <- to_categorical(test1)
test2 <- to_categorical(test2)
test3 <- to_categorical(test3)

train0 <- df[inTrain,-21]
test0 <- df[-inTrain,-21]
train0 <- to_categorical(train0)
test0 <- to_categorical(test0)
```

```{r}
set.seed(123) 

full.tree1 <- rpart(mod1, data=train1, control=rpart.control(cp=-1, minsplit=5))
full.tree2 <- rpart(mod1, data=train2, control=rpart.control(cp=-1, minsplit=5))
full.tree3 <- rpart(mod1, data=train3, control=rpart.control(cp=-1, minsplit=5))
full.tree4 <- rpart(mod1, data=train0, control=rpart.control(cp=-1, minsplit=5))

full.tree5 <- rpart(mod2, data=train1, control=rpart.control(cp=-1, minsplit=5))
full.tree6 <- rpart(mod2, data=train2, control=rpart.control(cp=-1, minsplit=5))
full.tree7 <- rpart(mod2, data=train3, control=rpart.control(cp=-1, minsplit=5))
full.tree8 <- rpart(mod2, data=train0, control=rpart.control(cp=-1, minsplit=5))
```

In the additional file, we can see a visualization of all full trees. They are pretty different. 

Now, we need to choose a complexity parameter (cp). In the additional file, we can see plots and information about misclassification error.

We will use 1SE (one standard error) rule: As the optimal tree, we choose the tree with the smallest number of splits (i.e. the smallest size) for which a misclassification error is within one standard error from the minimum misclassification error.

```{r}
cp.opt1 <- 0.125
cp.opt2 <- 0.083
cp.opt3 <- 0.083
cp.opt4 <- 0.041
cp.opt5 <- 0.083
cp.opt6 <- 0.083
cp.opt7 <- 0.166
cp.opt8 <- 0.062    
```

Now, we prune trees.

```{r}
full.tree1.pruned <- prune(full.tree1, cp = cp.opt1)
full.tree2.pruned <- prune(full.tree2, cp = cp.opt2)
full.tree3.pruned <- prune(full.tree3, cp = cp.opt3)
full.tree4.pruned <- prune(full.tree4, cp = cp.opt4)
full.tree5.pruned <- prune(full.tree5, cp = cp.opt5)
full.tree6.pruned <- prune(full.tree6, cp = cp.opt6)
full.tree7.pruned <- prune(full.tree7, cp = cp.opt7)
full.tree8.pruned <- prune(full.tree8, cp = cp.opt8)
```

In the additional file, we can find basic information about the trees. 

Now, let's visualize all the trees. They are pretty different: from a tree with only 2 splits to a tree with many splits.

```{r}
rpart.plot(full.tree1.pruned)
rpart.plot(full.tree2.pruned)
rpart.plot(full.tree3.pruned)
rpart.plot(full.tree4.pruned)
rpart.plot(full.tree5.pruned)
rpart.plot(full.tree6.pruned)
rpart.plot(full.tree7.pruned)
rpart.plot(full.tree8.pruned)
```

Predicted  posterior probabilities. Plot results where color - class.
Since we have the same splits, we can use the same n, p, colors.

There are false predictions in all models, not so few, especially many false negative values.

```{r}
pred.probs1 <- predict(full.tree1.pruned, newdata=test1, type = "prob")
pred.probs2 <- predict(full.tree2.pruned, newdata=test2, type = "prob")
pred.probs3 <- predict(full.tree3.pruned, newdata=test3, type = "prob")
pred.probs4 <- predict(full.tree4.pruned, newdata=test0, type = "prob")

pred.probs5 <- predict(full.tree5.pruned, newdata=test1, type = "prob")
pred.probs6 <- predict(full.tree6.pruned, newdata=test2, type = "prob")
pred.probs7 <- predict(full.tree7.pruned, newdata=test3, type = "prob")
pred.probs8 <- predict(full.tree8.pruned, newdata=test0, type = "prob")
```

```{r, echo=FALSE}
n <- nrow(test1)
classes <- as.factor(test1$Class)

pred.prob.plot1 <- data.frame(x = 1:n, probability = pred.probs1[,2], classes = classes)
pred.prob.plot2 <- data.frame(x = 1:n, probability = pred.probs2[,2], classes = classes)
pred.prob.plot3 <- data.frame(x = 1:n, probability = pred.probs3[,2], classes = classes)
pred.prob.plot4 <- data.frame(x = 1:n, probability = pred.probs4[,2], classes = classes)

pred.prob.plot5 <- data.frame(x = 1:n, probability = pred.probs5[,2], classes = classes)
pred.prob.plot6 <- data.frame(x = 1:n, probability = pred.probs6[,2], classes = classes)
pred.prob.plot7 <- data.frame(x = 1:n, probability = pred.probs7[,2], classes = classes)
pred.prob.plot8 <- data.frame(x = 1:n, probability = pred.probs8[,2], classes = classes)
```

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(data = pred.prob.plot1, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p2 <- ggplot(data = pred.prob.plot2, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p3 <- ggplot(data = pred.prob.plot3, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p4 <- ggplot(data = pred.prob.plot4, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p5 <- ggplot(data = pred.prob.plot5, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p6 <- ggplot(data = pred.prob.plot6, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p7 <- ggplot(data = pred.prob.plot7, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p8 <- ggplot(data = pred.prob.plot8, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4, nrow = 2)
```

Let's also plot histograms of predicted probabilities.

There are both good and bad results. There are histograms, where most probabilities are about 0 or 1, what we need. But there are also histograms, where a lot of values are located in the middle between 0 and 1.

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(pred.prob.plot1, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p2 <- ggplot(pred.prob.plot2, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p3 <- ggplot(pred.prob.plot3, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p4 <- ggplot(pred.prob.plot4, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p5 <- ggplot(pred.prob.plot5, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p6 <- ggplot(pred.prob.plot6, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p7 <- ggplot(pred.prob.plot7, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p8 <- ggplot(pred.prob.plot8, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4, nrow = 2)
```

Predicted class labels.

```{r}
pred.labels1 <- predict(full.tree1.pruned, newdata=test1, type = "class")
pred.labels2 <- predict(full.tree2.pruned, newdata=test2, type = "class")
pred.labels3 <- predict(full.tree3.pruned, newdata=test3, type = "class")
pred.labels4 <- predict(full.tree4.pruned, newdata=test0, type = "class")

pred.labels5 <- predict(full.tree5.pruned, newdata=test1, type = "class")
pred.labels6 <- predict(full.tree6.pruned, newdata=test2, type = "class")
pred.labels7 <- predict(full.tree7.pruned, newdata=test3, type = "class")
pred.labels8 <- predict(full.tree8.pruned, newdata=test0, type = "class")

real.labels <- test1$Class
```


```{r}
tree1 <- metrices(pred.labels1, real.labels)
tree2 <- metrices(pred.labels2, real.labels)
tree3 <- metrices(pred.labels3, real.labels)
tree4 <- metrices(pred.labels4, real.labels)

tree5 <- metrices(pred.labels5, real.labels)
tree6 <- metrices(pred.labels6, real.labels)
tree7 <- metrices(pred.labels7, real.labels)
tree8 <- metrices(pred.labels8, real.labels)

tree_metrices <- data.frame(tree1, tree2, tree3, tree4, tree5, tree6, tree7, tree8)
tree_metrices <- round(tree_metrices,2)
tree_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"), tree_metrices)

tree_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

## Bagging (bootstrap aggregating)

We can use both numeric and categorical data. Let's specify model formula. We use all attributes and also model without variables that are useless from EDA.

```{r}
mod1 <- Class ~ . - Class
mod2 <- Class ~ . - Class - Age - Steroid - AlkPhosphate - Sgot
```

Building random forest for different models and datasets (for different imputation methods and initial with missing values).

```{r}
set.seed(123) 

btree1 <- bagging(mod1, data=train1, nbagg=150, coob=TRUE, minsplit=2, cp=0)
btree2 <- bagging(mod1, data=train2, nbagg=150, coob=TRUE, minsplit=2, cp=0)
btree3 <- bagging(mod1, data=train3, nbagg=150, coob=TRUE, minsplit=2, cp=0)
btree4 <- bagging(mod1, data=train0, nbagg=150, coob=TRUE, minsplit=2, cp=0)

btree5 <- bagging(mod2, data=train1, nbagg=150, coob=TRUE, minsplit=2, cp=0)
btree6 <- bagging(mod2, data=train2, nbagg=150, coob=TRUE, minsplit=2, cp=0)
btree7 <- bagging(mod2, data=train3, nbagg=150, coob=TRUE, minsplit=2, cp=0)
btree8 <- bagging(mod2, data=train0, nbagg=150, coob=TRUE, minsplit=2, cp=0)
```

In the additional file, we can find basic information about the models.

Predicted  posterior probabilities. Plot results where color - class.
Since we have the same splits, we can use the same n, p, colors.

The results are not so good, because there are false predictions in all models and many probabilities in the middle.

```{r}
pred.probs1 <- predict(btree1, newdata=test1, type = "prob")
pred.probs2 <- predict(btree2, newdata=test2, type = "prob")
pred.probs3 <- predict(btree3, newdata=test3, type = "prob")
pred.probs4 <- predict(btree4, newdata=test0, type = "prob")

pred.probs5 <- predict(btree5, newdata=test1, type = "prob")
pred.probs6 <- predict(btree6, newdata=test2, type = "prob")
pred.probs7 <- predict(btree7, newdata=test3, type = "prob")
pred.probs8 <- predict(btree8, newdata=test0, type = "prob")
```

```{r, echo=FALSE}
n <- nrow(test1)
classes <- as.factor(test1$Class)

pred.prob.plot1 <- data.frame(x = 1:n, probability = pred.probs1[,2], classes = classes)
pred.prob.plot2 <- data.frame(x = 1:n, probability = pred.probs2[,2], classes = classes)
pred.prob.plot3 <- data.frame(x = 1:n, probability = pred.probs3[,2], classes = classes)
pred.prob.plot4 <- data.frame(x = 1:n, probability = pred.probs4[,2], classes = classes)

pred.prob.plot5 <- data.frame(x = 1:n, probability = pred.probs5[,2], classes = classes)
pred.prob.plot6 <- data.frame(x = 1:n, probability = pred.probs6[,2], classes = classes)
pred.prob.plot7 <- data.frame(x = 1:n, probability = pred.probs7[,2], classes = classes)
pred.prob.plot8 <- data.frame(x = 1:n, probability = pred.probs8[,2], classes = classes)
```

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(data = pred.prob.plot1, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p2 <- ggplot(data = pred.prob.plot2, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p3 <- ggplot(data = pred.prob.plot3, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p4 <- ggplot(data = pred.prob.plot4, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p5 <- ggplot(data = pred.prob.plot5, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p6 <- ggplot(data = pred.prob.plot6, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p7 <- ggplot(data = pred.prob.plot7, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p8 <- ggplot(data = pred.prob.plot8, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4, nrow = 2)
```

Let's also plot histograms of predicted probabilities.

As we saw before, the results are not so good: many values are about 0, but also a lot of values are located in the middle between 0 and 1.

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(pred.prob.plot1, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p2 <- ggplot(pred.prob.plot2, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p3 <- ggplot(pred.prob.plot3, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p4 <- ggplot(pred.prob.plot4, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p5 <- ggplot(pred.prob.plot5, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p6 <- ggplot(pred.prob.plot6, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p7 <- ggplot(pred.prob.plot7, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p8 <- ggplot(pred.prob.plot8, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4, nrow = 2)
```

Predicted class labels.

```{r}
pred.labels1 <- predict(btree1, newdata=test1, type = "class")
pred.labels2 <- predict(btree2, newdata=test2, type = "class")
pred.labels3 <- predict(btree3, newdata=test3, type = "class")
pred.labels4 <- predict(btree4, newdata=test0, type = "class")

pred.labels5 <- predict(btree5, newdata=test1, type = "class")
pred.labels6 <- predict(btree6, newdata=test2, type = "class")
pred.labels7 <- predict(btree7, newdata=test3, type = "class")
pred.labels8 <- predict(btree8, newdata=test0, type = "class")

real.labels <- test1$Class
```


```{r}
btree1 <- metrices(pred.labels1, real.labels)
btree2 <- metrices(pred.labels2, real.labels)
btree3 <- metrices(pred.labels3, real.labels)
btree4 <- metrices(pred.labels4, real.labels)

btree5 <- metrices(pred.labels5, real.labels)
btree6 <- metrices(pred.labels6, real.labels)
btree7 <- metrices(pred.labels7, real.labels)
btree8 <- metrices(pred.labels8, real.labels)

btree_metrices <- data.frame(btree1, btree2, btree3, btree4, btree5, btree6, btree7, btree8)
btree_metrices <- round(btree_metrices,2)
btree_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"), btree_metrices)

btree_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

## Boosting

We can use both numeric and categorical data. Let's specify model formula. We use all attributes and also model without variables that are useless from EDA.

```{r}
mod1 <- Class ~ . - Class
mod2 <- Class ~ . - Class - Age - Steroid - AlkPhosphate - Sgot
```

Building random forest for different models and datasets (for different imputation methods).

```{r}
set.seed(123) 

boost1 <- ada(mod1, data=train1, iter = 10)
boost2 <- ada(mod1, data=train2, iter = 10)
boost3 <- ada(mod1, data=train3, iter = 10)
boost4 <- ada(mod1, data=train0, iter = 10)

boost5 <- ada(mod2, data=train1, iter = 10)
boost6 <- ada(mod2, data=train2, iter = 10)
boost7 <- ada(mod2, data=train3, iter = 10)
boost8 <- ada(mod2, data=train0, iter = 10)
```

In the additional file, we can find basic information about all models.

Predicted  posterior probabilities. Plot results where color - class.
Since we have the same splits, we can use the same n, p, colors.

The results are not so good, because there are false predictions in all models and many probabilities in the middle.

```{r}
pred.probs1 <- predict(boost1, newdata=test1, type = "prob")
pred.probs2 <- predict(boost2, newdata=test2, type = "prob")
pred.probs3 <- predict(boost3, newdata=test3, type = "prob")
pred.probs4 <- predict(boost4, newdata=test0, type = "prob")

pred.probs5 <- predict(boost5, newdata=test1, type = "prob")
pred.probs6 <- predict(boost6, newdata=test2, type = "prob")
pred.probs7 <- predict(boost7, newdata=test3, type = "prob")
pred.probs8 <- predict(boost8, newdata=test0, type = "prob")
```

```{r, echo=FALSE}
n <- nrow(test1)
classes <- as.factor(test1$Class)

pred.prob.plot1 <- data.frame(x = 1:n, probability = pred.probs1[,2], classes = classes)
pred.prob.plot2 <- data.frame(x = 1:n, probability = pred.probs2[,2], classes = classes)
pred.prob.plot3 <- data.frame(x = 1:n, probability = pred.probs3[,2], classes = classes)
pred.prob.plot4 <- data.frame(x = 1:n, probability = pred.probs4[,2], classes = classes)

pred.prob.plot5 <- data.frame(x = 1:n, probability = pred.probs5[,2], classes = classes)
pred.prob.plot6 <- data.frame(x = 1:n, probability = pred.probs6[,2], classes = classes)
pred.prob.plot7 <- data.frame(x = 1:n, probability = pred.probs7[,2], classes = classes)
pred.prob.plot8 <- data.frame(x = 1:n, probability = pred.probs8[,2], classes = classes)
```

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(data = pred.prob.plot1, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p2 <- ggplot(data = pred.prob.plot2, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p3 <- ggplot(data = pred.prob.plot3, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p4 <- ggplot(data = pred.prob.plot4, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p5 <- ggplot(data = pred.prob.plot5, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p6 <- ggplot(data = pred.prob.plot6, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p7 <- ggplot(data = pred.prob.plot7, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p8 <- ggplot(data = pred.prob.plot8, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4, nrow = 2)
```

Let's also plot histograms of predicted probabilities.

As we saw before, the results are not so good: many values are about 0, but also a lot of values are located in the middle between 0 and 1.

```{r, fig.height = 5, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(pred.prob.plot1, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p2 <- ggplot(pred.prob.plot2, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p3 <- ggplot(pred.prob.plot3, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p4 <- ggplot(pred.prob.plot4, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p5 <- ggplot(pred.prob.plot5, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p6 <- ggplot(pred.prob.plot6, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p7 <- ggplot(pred.prob.plot7, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p8 <- ggplot(pred.prob.plot8, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4, nrow = 2)
```

Predicted class labels.

```{r}
pred.labels1 <- predict(boost1, newdata=test1)
pred.labels2 <- predict(boost2, newdata=test2)
pred.labels3 <- predict(boost3, newdata=test3)
pred.labels4 <- predict(boost4, newdata=test0)

pred.labels5 <- predict(boost5, newdata=test1)
pred.labels6 <- predict(boost6, newdata=test2)
pred.labels7 <- predict(boost7, newdata=test3)
pred.labels8 <- predict(boost8, newdata=test0)

real.labels <- test1$Class
```


```{r}
boost.1 <- metrices(pred.labels1, real.labels)
boost.2 <- metrices(pred.labels2, real.labels)
boost.3 <- metrices(pred.labels3, real.labels)
boost.4 <- metrices(pred.labels4, real.labels)

boost.5 <- metrices(pred.labels5, real.labels)
boost.6 <- metrices(pred.labels6, real.labels)
boost.7 <- metrices(pred.labels7, real.labels)
boost.8 <- metrices(pred.labels8, real.labels)

boost_metrices <- data.frame(boost.1, boost.2, boost.3, boost.4, boost.5, boost.6, boost.7, boost.8)
boost_metrices <- round(boost_metrices,2)
boost_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"), boost_metrices)

boost_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```


## Random forest

We can use both numeric and categorical data. Let's specify model formula. We use all attributes and also model without variables that are useless from EDA. Also we need number of features.

```{r}
mod1 <- Class ~ . - Class
mod2 <- Class ~ . - Class - Age - Steroid - AlkPhosphate - Sgot

p1 <-  ncol(test1) - 1
p2 <-  ncol(test1) - 5
```

Building random forest for different models, parameters (ntree - number of trees, mtry - number of randomly selected features), and datasets (for different imputation methods).

```{r}
set.seed(123) 

rf1 <- randomForest(mod1, data=train1, ntree=500, mtry=p1, importance=TRUE)
rf2 <- randomForest(mod1, data=train2, ntree=500, mtry=p1, importance=TRUE)
rf3 <- randomForest(mod1, data=train3, ntree=500, mtry=p1, importance=TRUE)

rf4 <- randomForest(mod1, data=train1, ntree=500, mtry=sqrt(p1), importance=TRUE)
rf5 <- randomForest(mod1, data=train2, ntree=500, mtry=sqrt(p1), importance=TRUE)
rf6 <- randomForest(mod1, data=train3, ntree=500, mtry=sqrt(p1), importance=TRUE)

rf7 <- randomForest(mod2, data=train1, ntree=500, mtry=p2, importance=TRUE)
rf8 <- randomForest(mod2, data=train2, ntree=500, mtry=p2, importance=TRUE)
rf9 <- randomForest(mod2, data=train3, ntree=500, mtry=p2, importance=TRUE)

rf10 <- randomForest(mod2, data=train1, ntree=500, mtry=sqrt(p2), importance=TRUE)
rf11 <- randomForest(mod2, data=train2, ntree=500, mtry=sqrt(p2), importance=TRUE)
rf12 <- randomForest(mod2, data=train3, ntree=500, mtry=sqrt(p2), importance=TRUE)
```

In the additional file, we can find basic information and classification error plot for the random forests.

Now, let's create a plot that displays the importance of each predictor variable in the final model. The x-axis displays the average increase in node purity of the regression trees based on splitting on the various predictors displayed on the y-axis. From the plot we can see that Protime is the most important predictor variable.

In the additional file, we can see variable importance ranking for other random forest models. The results are different, but for each model, Protime is in the first place. 

```{r}
varImpPlot(rf1, main = "Variable Importance Plot")
```

Predicted  posterior probabilities. Plot results where color - class.
Since we have the same splits, we can use the same n, p, colors.

The results are not so good, because there are false predictions in all models and many probabilities in the middle.

```{r}
pred.probs1 <- predict(rf1, newdata=test1, type = "prob")
pred.probs2 <- predict(rf2, newdata=test2, type = "prob")
pred.probs3 <- predict(rf3, newdata=test3, type = "prob")

pred.probs4 <- predict(rf4, newdata=test1, type = "prob")
pred.probs5 <- predict(rf5, newdata=test2, type = "prob")
pred.probs6 <- predict(rf6, newdata=test3, type = "prob")

pred.probs7 <- predict(rf7, newdata=test1, type = "prob")
pred.probs8 <- predict(rf8, newdata=test2, type = "prob")
pred.probs9 <- predict(rf9, newdata=test3, type = "prob")

pred.probs10 <- predict(rf10, newdata=test1, type = "prob")
pred.probs11 <- predict(rf11, newdata=test2, type = "prob")
pred.probs12 <- predict(rf12, newdata=test3, type = "prob")
```

```{r, echo=FALSE}
n <- nrow(test1)
classes <- as.factor(test1$Class)

pred.prob.plot1 <- data.frame(x = 1:n, probability = pred.probs1[,2], classes = classes)
pred.prob.plot2 <- data.frame(x = 1:n, probability = pred.probs2[,2], classes = classes)
pred.prob.plot3 <- data.frame(x = 1:n, probability = pred.probs3[,2], classes = classes)

pred.prob.plot4 <- data.frame(x = 1:n, probability = pred.probs4[,2], classes = classes)
pred.prob.plot5 <- data.frame(x = 1:n, probability = pred.probs5[,2], classes = classes)
pred.prob.plot6 <- data.frame(x = 1:n, probability = pred.probs6[,2], classes = classes)

pred.prob.plot7 <- data.frame(x = 1:n, probability = pred.probs7[,2], classes = classes)
pred.prob.plot8 <- data.frame(x = 1:n, probability = pred.probs8[,2], classes = classes)
pred.prob.plot9 <- data.frame(x = 1:n, probability = pred.probs9[,2], classes = classes)

pred.prob.plot10 <- data.frame(x = 1:n, probability = pred.probs10[,2], classes = classes)
pred.prob.plot11 <- data.frame(x = 1:n, probability = pred.probs11[,2], classes = classes)
pred.prob.plot12 <- data.frame(x = 1:n, probability = pred.probs12[,2], classes = classes)
```

```{r, fig.height = 10, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(data = pred.prob.plot1, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p2 <- ggplot(data = pred.prob.plot2, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p3 <- ggplot(data = pred.prob.plot3, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p4 <- ggplot(data = pred.prob.plot4, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p5 <- ggplot(data = pred.prob.plot5, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p6 <- ggplot(data = pred.prob.plot6, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p7 <- ggplot(data = pred.prob.plot7, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p8 <- ggplot(data = pred.prob.plot8, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p9 <- ggplot(data = pred.prob.plot9, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

p10 <- ggplot(data = pred.prob.plot10, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p11 <- ggplot(data = pred.prob.plot11, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))
p12 <- ggplot(data = pred.prob.plot12, aes(x = x, y = probability)) +
    geom_point(aes(color = classes))

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, ncol = 3, nrow = 4)
```

Let's also plot histograms of predicted probabilities.

As we saw before, the results are not so good: many values are about 0, but also a lot of values are located in the middle between 0 and 1.

```{r, fig.height = 10, fig.width = 10, fig.align = 'center', warning=FALSE, message=FALSE}
p1 <- ggplot(pred.prob.plot1, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p2 <- ggplot(pred.prob.plot2, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p3 <- ggplot(pred.prob.plot3, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p4 <- ggplot(pred.prob.plot4, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p5 <- ggplot(pred.prob.plot5, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p6 <- ggplot(pred.prob.plot6, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p7 <- ggplot(pred.prob.plot7, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p8 <- ggplot(pred.prob.plot8, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p9 <- ggplot(pred.prob.plot9, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

p10 <- ggplot(pred.prob.plot10, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p11 <- ggplot(pred.prob.plot11, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)
p12 <- ggplot(pred.prob.plot12, aes(x = probability)) +
   geom_histogram(color='#f8766d', fill='#f8766d', bins = 5)

ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, ncol = 3, nrow = 4)
```

Predicted class labels.

```{r}
pred.labels1 <- predict(rf1, newdata=test1, type = "class")
pred.labels2 <- predict(rf2, newdata=test2, type = "class")
pred.labels3 <- predict(rf3, newdata=test3, type = "class")

pred.labels4 <- predict(rf4, newdata=test1, type = "class")
pred.labels5 <- predict(rf5, newdata=test2, type = "class")
pred.labels6 <- predict(rf6, newdata=test3, type = "class")

pred.labels7 <- predict(rf7, newdata=test1, type = "class")
pred.labels8 <- predict(rf8, newdata=test2, type = "class")
pred.labels9 <- predict(rf9, newdata=test3, type = "class")

pred.labels10 <- predict(rf10, newdata=test1, type = "class")
pred.labels11 <- predict(rf11, newdata=test2, type = "class")
pred.labels12 <- predict(rf12, newdata=test3, type = "class")

real.labels <- test1$Class
```


```{r}
rf.1 <- metrices(pred.labels1, real.labels)
rf.2 <- metrices(pred.labels2, real.labels)
rf.3 <- metrices(pred.labels3, real.labels)

rf.4 <- metrices(pred.labels4, real.labels)
rf.5 <- metrices(pred.labels5, real.labels)
rf.6 <- metrices(pred.labels6, real.labels)

rf.7 <- metrices(pred.labels7, real.labels)
rf.8 <- metrices(pred.labels8, real.labels)
rf.9 <- metrices(pred.labels9, real.labels)

rf.10 <- metrices(pred.labels10, real.labels)
rf.11 <- metrices(pred.labels11, real.labels)
rf.12 <- metrices(pred.labels12, real.labels)

rf_metrices <- data.frame(rf.1, rf.2, rf.3, rf.4, rf.5, rf.6, rf.7, rf.8, rf.9, rf.10, rf.11, rf.12)
rf_metrices <- round(rf_metrices,2)
rf_metrices <- cbind(metric=c("Accuracy", "Recall", "Precision", "F-measure"), rf_metrices)

rf_metrices %>% 
    flextable() %>% 
    theme_box() %>% 
    autofit()
```

<!-- ## Comparison of accuracy (10-fold Cross-Validation scheme) -->
<!-- ###################################################################### -->

<!-- mypredict.rpart <- function(object, newdata)  predict(object, newdata=newdata, type="class") -->

<!-- (error.tree         <- (errorest(Type~., data=Glass, model=rpart, predict=mypredict.rpart))) -->
<!-- (error.bagging      <- (errorest(Type~., data=Glass, model=bagging))) -->
<!-- (error.randomForest <- (errorest(Type~., data=Glass, model=randomForest))) -->

<!-- ### Method -->
<!-- ### Confusion matrix -->
<!-- ### Cross-Validation (CV)  -->
<!-- #### k-fold cross-validation -->
<!-- #### leave-one-out -->
<!-- ### Bootstrap-based methods -->
<!-- #### leave-one-out bootstrap, -->
<!-- #### .632 estimator, -->
<!-- #### .632+ estimator.  -->
<!-- ###	ROC-curve -->


