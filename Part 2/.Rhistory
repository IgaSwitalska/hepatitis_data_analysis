library(dplyr)
library(visdat)
library(caret)
library(RANN)
library(corrplot)
library(plotly)
library(ggplot2)
library(resample)
library(DataExplorer)
library(imputeMulti)
library(mice)
library(rmarkdown)
library(repr)
library(tidyverse)
library(flextable)
library(imbalance)
library(cowplot)
library(ggpubr)
library(MASS)
library(GGally)
library(mlbench)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ipred)
library(ada)
library(class)
library(cluster)
df <- read.table("hepatitis.data", sep = ",")
setwd("C:/iga/studia II/semestr 2/data_minig/hepatitis_data_analysis/Part 2")
df <- read.table("hepatitis.data", sep = ",")
colnames(df) <- c("Class","Age","Sex","Steroid","Antivirals","Fatigue","Malaise","Anorexia","LiverBig","LiverFirm","SpleenPalpable","Spiders","Ascites","Varices","Bilirubin","AlkPhosphate","Sgot","Albumin","Protime","Histology")
df[df == "?"] <- NA
df <- mutate_all(df, function(x) as.numeric(as.character(x)))
categorical <- c(1, 3:14, 20)
df[, categorical] <- replace(df[, categorical], df[, categorical] == 2, 0)
df[, categorical] <-  lapply(df[, categorical], as.factor)
set.seed(123)
df.new <- mutate_all(df, function(x) as.numeric(as.character(x)))
data_transform <- preProcess(df.new, method = "knnImpute")
df1 <- predict(data_transform, df.new)
df1.standarized <- df1
unstandarize <- function(data){
for (i in 1:20) {
column <- df.new[, i]
if (i %in% c(15, 18)) {
data[, i] <- data[, i] * sd(na.omit(column)) + mean(na.omit(column))
} else {
data[, i] <- round(data[, i] * sd(na.omit(column)) + mean(na.omit(column)))
}
}
return(data)
}
df1 <- unstandarize(df1)
################################################################################
# Remove unnecessary features
df.mds <- subset(df1, col=-c("Class"))
################################################################################
# Remove unnecessary features
df.mds <- df1[,2:20]
View(df.mds)
# Derive dissimilarities between objects
dissimilarities <- daisy(df.mds, stand = T)
dis.matrix <- as.matrix(dissimilarities)
dissimilarities2 <- daisy(df.mds, stand = T, ,metric = "gower")
dissimilarities2 <- daisy(df.mds, stand = T,metric = "gower")
d.max <- 6
scree.plot <- function(d, k) {
stresses <- sammon(d, k = k)$stress
for(i in rev(seq(k-1)))
stresses <- append(stresses, sammon(d, k = i)$stress)
plot(seq(k),rev(stresses), type="b", xaxp=c(1,k, k-1), ylab="Stress", xlab="Number of dimensions")
}
scree.plot(dissimilarities, k = d.max <- 6)
stress.vec <- numeric(d.max)
par(mfrow=c(3,2))
for (d in 1:d.max)
or (d in 1:d.max)
for (d in 1:d.max)
{
mds.k <- sammon(dis.matrix, k = d)
STRESS <- mds.k$stress
stress.vec[d] <- STRESS
# Shepard diagram
shep <- Shepard(dissimilarities, mds.k$points, p=d)
plot(shep, pch=".", main=paste0("Shepard diagram (d=",d,")"),
cex=0.5, xlab="original distance",  ylab="distance after MDS mapping")
lines(shep$x, shep$yf, type = "S", col="red", lty=2)
grid()
legend(x="topleft",legend=paste("STRESS = ",signif(STRESS,3)), bg="azure2")
}
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
df.mds <- data.frame(df.mds)
df.mds$class <- df1$Class
df.mds
inTrain <- createDataPartition(y=df.mds$class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,-3]
test <- df1[-inTrain,-3]
train <- df.mds[inTrain,]
test <- df1[-inTrain,]
barolt(train$Class)
barplot(train$class)
train$class
prop.table(table(train$Class))
df.mds$class <- lapply(df1$Class, as.factor)
df.mds
lapply(df1$Class, as.factor)
lapply(df1[,1], as.factor)
df.mds$class <- df1$Class
df.mds
df.mds[,3] <- lapply(df.mds[,3], as.factor)
lapply(df.mds[,3], as.factor)
lapply(df[, categorical], as.factor)
df <- read.table("hepatitis.data", sep = ",")
df.mds[,3]
df.mds
df <- read.table("hepatitis.data", sep = ",")
colnames(df) <- c("Class","Age","Sex","Steroid","Antivirals","Fatigue","Malaise","Anorexia","LiverBig","LiverFirm","SpleenPalpable","Spiders","Ascites","Varices","Bilirubin","AlkPhosphate","Sgot","Albumin","Protime","Histology") # nolint
df[df == "?"] <- NA
df <- mutate_all(df, function(x) as.numeric(as.character(x)))
categorical <- c(1, 3:14, 20)
df[, categorical] <- replace(df[, categorical], df[, categorical] == 2, 0)
df[, categorical] <-  lapply(df[, categorical], as.factor)
set.seed(123)
df.new <- mutate_all(df, function(x) as.numeric(as.character(x)))
data_transform <- preProcess(df.new, method = "knnImpute")
df1 <- predict(data_transform, df.new)
unstandarize <- function(data){
for (i in 1:20) {
column <- df.new[, i]
if (i %in% c(15, 18)) {
data[, i] <- data[, i] * sd(na.omit(column)) + mean(na.omit(column))
} else {
data[, i] <- round(data[, i] * sd(na.omit(column)) + mean(na.omit(column)))
}
}
return(data)
}
df1.standarized <- df1
df1 <- unstandarize(df1)
# Remove unnecessary features
df.mds <- subset(df1, col=-c("Class"))
# Derive dissimilarities between objects
dissimilarities <- daisy(df.mds, stand = T)
dis.matrix <- as.matrix(dissimilarities)
d.max <- 6
scree.plot <- function(d, k) {
stresses <- sammon(d, k = k)$stress
for(i in rev(seq(k-1)))
stresses <- append(stresses, sammon(d, k = i)$stress)
plot(seq(k),rev(stresses), type="b", xaxp=c(1,k, k-1), ylab="Stress", xlab="Number of dimensions")
}
scree.plot(dissimilarities, k = d.max <- 6)
stress.vec <- numeric(d.max)
par(mfrow=c(3,2))
for (d in 1:d.max)
{
mds.k <- sammon(dis.matrix, k = d)
STRESS <- mds.k$stress
stress.vec[d] <- STRESS
# Shepard diagram
shep <- Shepard(dissimilarities, mds.k$points, p=d)
plot(shep, pch=".", main=paste0("Shepard diagram (d=",d,")"),
cex=0.5, xlab="original distance",  ylab="distance after MDS mapping")
lines(shep$x, shep$yf, type = "S", col="red", lty=2)
grid()
legend(x="topleft",legend=paste("STRESS = ",signif(STRESS,3)), bg="azure2")
}
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
set.seed(123)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
df.mds <- data.frame(df.mds)
df.mds$class <- df1$Class
df.mds
df.mds[,3] <- lapply(df.mds[,3], as.factor)
df.mds
df.mds$class <- df1$Class
df.mds
df1$Class
lapply(df1$Class, as.factor)
df[,3]
df1[,3]
df1$Class
lapply(df1$Class, as.factor)
df1$Class
df1[,1]
lapply(df1[,1], as.factor)
lapply(df1[,c(1)], as.factor)
df.mds[,3]
as.factor(df.mds$class)
df.mds$class <- as.factor(df1$Class)
df.mds
inTrain <- createDataPartition(y=df.mds$class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,]
test <- df1[-inTrain,]
prop.table(table(train$Class))
df.mds
inTrain
train
test
test <- df.mds[-inTrain,]
test
table(train$Class)
prop.table(table(train$class))
df.mds <- sammon(dis.matrix, k = 2)$points
df.mds <- data.frame(df.mds)
df.mds$Class <- as.factor(df1$Class)
df.mds
inTrain <- createDataPartition(y=df.mds$class, times=1, p=0.75, list=FALSE)
inTrain <- createDataPartition(y=df.mds$Class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,]
test <- df.mds[-inTrain,]
prop.table(table(train$Class))
n_new <- sum(train$Class == 0) - sum(train$Class == 1)
n_new
newMWMOTE <- mwmote(train, numInstances = n_new)
train.balanced <- rbind(train, newMWMOTE1)
train.balanced <- rbind(train, newMWMOTE)
prop.table(table(train.balanced$Class))
plotComparison(train, train.balanced)
plotComparison(train, train.balanced, attrs = c("X1", "X2"))
model.lm <- lm(Class~., data=train.balanced)
train.balanced
as.numeric(train.balanced$X1)
type(as.numeric(train.balanced$X1))
typeof(train.balanced$X1)
typeof(as.numeric(train.balanced$X1))
<- lm(Class~., data=train.balanced)
model.lm
df1
# Remove unnecessary features
df1[, categorical] <-  lapply(df1[, categorical], as.factor)
df1
df.mds <- subset(df1, col=-c("Class"))
df.mds
df.mds <- subset(df1, col=-c("Class"))
# df.mds <- subset(df1, col=-c("Class"))
df.mds <- df1[,2:20]
df.mds
# Derive dissimilarities between objects
dissimilarities <- daisy(df.mds, stand = T)
dis.matrix <- as.matrix(dissimilarities)
d.max <- 6
scree.plot <- function(d, k) {
stresses <- sammon(d, k = k)$stress
for(i in rev(seq(k-1)))
stresses <- append(stresses, sammon(d, k = i)$stress)
plot(seq(k),rev(stresses), type="b", xaxp=c(1,k, k-1), ylab="Stress", xlab="Number of dimensions")
}
scree.plot(dissimilarities, k = d.max <- 6)
stress.vec <- numeric(d.max)
par(mfrow=c(3,2))
for (d in 1:d.max)
exit()
for (d in 1:d.max)
{
mds.k <- sammon(dis.matrix, k = d)
STRESS <- mds.k$stress
stress.vec[d] <- STRESS
# Shepard diagram
shep <- Shepard(dissimilarities, mds.k$points, p=d)
plot(shep, pch=".", main=paste0("Shepard diagram (d=",d,")"),
cex=0.5, xlab="original distance",  ylab="distance after MDS mapping")
lines(shep$x, shep$yf, type = "S", col="red", lty=2)
grid()
legend(x="topleft",legend=paste("STRESS = ",signif(STRESS,3)), bg="azure2")
}
set.seed(123)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
df.mds <- data.frame(df.mds)
df.mds$Class <- as.factor(df1$Class)
df.mds
set.seed(123)
inTrain <- createDataPartition(y=df.mds$Class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,]
test <- df.mds[-inTrain,]
n_new <- sum(train$Class == 0) - sum(train$Class == 1)
newMWMOTE <- mwmote(train, numInstances = n_new)
train.balanced <- rbind(train, newMWMOTE)
prop.table(table(train.balanced$Class))
plotComparison(train, train.balanced, attrs = c("X1", "X2"))
model.lm <- lm(Class~., data=train.balanced)
train.balanced$Class <- as.numeric(train.balanced$Class)
model.lm <- lm(Class~., data=train.balanced)
set.seed(124)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
set.seed(1)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
set.seed(123)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
slope <- function(model){
-model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
-(model$coefficients[1]-0.5)/model$coefficients[3]
}
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
slope(model.lm)
model.lm
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
+ geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
+ geom_point(size=1)
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
model.lm <- lm(Class~X1+X2, data=train.balanced)
slope <- function(model){
-model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
-(model$coefficients[1]-0.5)/model$coefficients[3]
}
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
model.lm$coefficients
slope(model.lm)
intercept(model.lm)
model.lm$coefficients
model
model.lm$coefficients
model.lm$coefficients
slope <- -model.lm$coefficients[2]/model.lm$coefficients[3]
slope
model.lm$coefficients[2]
model.lm$coefficients[3]
train.balanced
train.balanced$Class <- replace(train.balanced$Class, train.balanced$Class == 2, 0)
train.balanced$Class
model.lm <- lm(Class~X1+X2, data=train.balanced)
model.lm$coefficients
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
slope <- function(model){
-model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
-(model$coefficients[1]-0.5)/model$coefficients[3]
}
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
lr_pred <- function(model,test,thr){
pred <- predict(model, test)
pred[pred > thr] = 1
pred[pred < thr] = 0
return(pred)
}
lda_pred <- function(model,test,thr){
pred <- predict(model,test)$posterior[,2]
pred[pred > thr] = 1
pred[pred < thr] = 0
return(pred)
}
metrices <- function(pred, real){
confusion.matrix <- confusionMatrix(table(pred, real), positive = "1")
statistics <- confusion.matrix.results(confusion.matrix)
p <- precision(table(pred, real), relevant = "1")
return(c(statistics,p))
}
metrices(lr_pred(model.lm, test, 0.5), test$Class)
test
test$Class <- as.numeric(test$Class)
test
test$Class <- replace(test$Class, test$Class == 2, 0)
test <- df.mds[-inTrain,]
test
df.mds
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
df.mds <- data.frame(df.mds)
df.mds$Class <- as.factor(df1$Class)
df.mds
train <- df.mds[inTrain,]
test <- df.mds[-inTrain,]
test
as.numeric(test$Class)
df.mds
n_new <- sum(train$Class == 0) - sum(train$Class == 1)
newMWMOTE <- mwmote(train, numInstances = n_new)
train.balanced <- rbind(train, newMWMOTE)
prop.table(table(train.balanced$Class))
plotComparison(train, train.balanced, attrs = c("X1", "X2"))
train.balanced$Class <- as.numeric(train.balanced$Class)
train.balanced$Class <- train.balanced$Class - 1
test$Class <- as.numeric(test$Class)
test$Class <- test$Class - 1
test$Class
model.lm <- lm(Class~X1+X2, data=train.balanced)
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
test <- df.mds[-inTrain,]
test.num <- test
test.num$Class <- as.numeric(test.num$Class)
test.num$Class <- test.num$Class - 1
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
metrices(lr_pred(model.lm, test.num, 0.5), test.num$Class)
confusion.matrix.results <- function(cm){
overall <- cm$overall[1:2]
gg
exit()
exitt()
}
confusion.matrix.results <- function(cm){
byClass <- cm$byClass[c(1, 2, 7, 11)]
overall <- cm$overall[1:2]
statistics <- append(overall, byClass)
return(statistics)
}
metrices <- function(pred, real){
confusion.matrix <- confusionMatrix(table(pred, real), positive = "1")
statistics <- confusion.matrix.results(confusion.matrix)
p <- precision(table(pred, real), relevant = "1")
return(c(statistics,p))
}
metrices(lr_pred(model.lm, test.num, 0.5), test.num$Class)
model.logit <-  glm(Class~X1+X2, data=train.balanced, family=binomial(link="logit"))
pred.prob <- predict(model.logit, test.num, type = "response")
pred.prob.plot <- data.frame(x = 1:n, probability = pred.prob, classes = test$Class)
n <- nrow(test.num)
pred.prob.plot <- data.frame(x = 1:n, probability = pred.prob, classes = test$Class)
ggplot(data = pred.prob.plot, aes(x = x, y = probability)) +
geom_point(aes(color = test$Class))
