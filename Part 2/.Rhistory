library(dplyr)
library(caret)
library(MASS)
library(cluster)
setwd("C:/iga/studia II/semestr 2/data_minig/hepatitis_data_analysis/Part 2")
df <- read.table("hepatitis.data", sep = ",")
colnames(df) <- c("Class","Age","Sex","Steroid","Antivirals","Fatigue","Malaise","Anorexia","LiverBig","LiverFirm","SpleenPalpable","Spiders","Ascites","Varices","Bilirubin","AlkPhosphate","Sgot","Albumin","Protime","Histology") # nolint
df[df == "?"] <- NA
df <- mutate_all(df, function(x) as.numeric(as.character(x)))
categorical <- c(1, 3:14, 20)
df[, categorical] <- replace(df[, categorical], df[, categorical] == 2, 0)
df[, categorical] <-  lapply(df[, categorical], as.factor)
set.seed(123)
df.new <- mutate_all(df, function(x) as.numeric(as.character(x)))
data_transform <- preProcess(df.new, method = "knnImpute")
df1 <- predict(data_transform, df.new)
unstandarize <- function(data){
for (i in 1:20) {
column <- df.new[, i]
if (i %in% c(15, 18)) {
data[, i] <- data[, i] * sd(na.omit(column)) + mean(na.omit(column))
} else {
data[, i] <- round(data[, i] * sd(na.omit(column)) + mean(na.omit(column)))
}
}
return(data)
}
df1.standarized <- df1
df1 <- unstandarize(df1)
df1[, categorical] <-  lapply(df1[, categorical], as.factor)
# df.mds <- subset(df1, col=-c("Class"))
df.mds <- df1[,2:20]
# Derive dissimilarities between objects
dissimilarities <- daisy(df.mds, stand = T)
dis.matrix <- as.matrix(dissimilarities)
d.max <- 6
scree.plot <- function(d, k) {
stresses <- sammon(d, k = k)$stress
for(i in rev(seq(k-1)))
stresses <- append(stresses, sammon(d, k = i)$stress)
plot(seq(k),rev(stresses), type="b", xaxp=c(1,k, k-1), ylab="Stress", xlab="Number of dimensions")
}
scree.plot(dissimilarities, k = d.max <- 6)
set.seed(123)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
df.mds <- data.frame(df.mds)
df.mds$Class <- as.factor(df1$Class)
df.mds
set.seed(123)
inTrain <- createDataPartition(y=df.mds$Class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,]
test <- df.mds[-inTrain,]
n_new <- sum(train$Class == 0) - sum(train$Class == 1)
newMWMOTE <- mwmote(train, numInstances = n_new)
library(imbalance)
set.seed(123)
inTrain <- createDataPartition(y=df.mds$Class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,]
test <- df.mds[-inTrain,]
n_new <- sum(train$Class == 0) - sum(train$Class == 1)
newMWMOTE <- mwmote(train, numInstances = n_new)
train.balanced <- rbind(train, newMWMOTE)
prop.table(table(train.balanced$Class))
plotComparison(train, train.balanced, attrs = c("X1", "X2"))
train.balanced$Class <- as.numeric(train.balanced$Class)
train.balanced$Class <- train.balanced$Class - 1
test.num <- test
test.num$Class <- as.numeric(test.num$Class)
test.num$Class <- test.num$Class - 1
# some useful functions
slope <- function(model){
-model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
-(model$coefficients[1]-0.5)/model$coefficients[3]
}
lr_pred <- function(model,test,thr){
pred <- predict(model, test)
pred[pred > thr] = 1
pred[pred < thr] = 0
return(pred)
}
lda_pred <- function(model,test,thr){
pred <- predict(model,test)$posterior[,2]
pred[pred > thr] = 1
pred[pred < thr] = 0
return(pred)
}
confusion.matrix.results <- function(cm){
byClass <- cm$byClass[c(1, 2, 7, 11)]
overall <- cm$overall[1:2]
statistics <- append(overall, byClass)
return(statistics)
}
metrices <- function(pred, real){
confusion.matrix <- confusionMatrix(table(pred, real), positive = "1")
statistics <- confusion.matrix.results(confusion.matrix)
p <- precision(table(pred, real), relevant = "1")
return(c(statistics,p))
}
model.lm <- lm(Class~X1+X2, data=train.balanced)
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
metrices(lr_pred(model.lm, test.num, 0.5), test.num$Class)
n <- nrow(test.num)
model.logit <-  glm(Class~X1+X2, data=train.balanced, family=binomial(link="logit"))
pred.prob <- predict(model.logit, test.num, type = "response")
pred.prob.plot <- data.frame(x = 1:n, probability = pred.prob, classes = test$Class)
ggplot(data = pred.prob.plot, aes(x = x, y = probability)) +
geom_point(aes(color = test$Class))
prob.to.labels <- function(probs, cutoff){
classes <- rep("0",length(probs))
classes[probs>cutoff] <- "1"
return(as.factor(classes))
}
pred.labels <- prob.to.labels(probs=pred.prob, cutoff=0.5)
real.labels <- test.num$Class
cm <- confusionMatrix(table(pred.labels1, real.labels), positive = "1")
cm <- confusionMatrix(table(pred.labels, real.labels), positive = "1")
logit <- confusion.matrix.results(cm)
logit
k.grid <- data.frame(k=1:25)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)
train.balanced1
train.balanced
set.seed(123)
k.grid <- data.frame(k=1:25)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)
knn.model <- train(train.balanced, as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
mean(train.balanced)
mean(train.balanced$X1)
mean(train.balanced$X2)
std(train.balanced$X2)
st(train.balanced$X2)
stdr(train.balanced$X2)
sd(train.balanced$X2)
sd(train.balanced$X2)
sd(train.balanced$X1)
knn.model <- train(train.balanced[c(1,2)], as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
knn.model <- train(train.balanced[c(1,2)], as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
dataTransform <- preProcess(train.balanced, method=c("center", "scale"))
train.balanced.std <- predict(dataTransform, train.balanced1)
train.balanced.std <- predict(dataTransform, train.balanced)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)
knn.model <- train(train.balanced.std[c(1,2)], as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
knn.model <- train(train.balanced.std[c(1,2)], as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
sd(train.balanced.std)
train.balanced.std
sd(train.balanced.std$X1)
sd(train.balanced.std$X2)
ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
library(NbClust)
source("fviz_nbclust_fixed.R") # fixes the bug in factoextra::fviz_nbclust(...)
df.features <- df1.standarized[,2:20]
dm <- daisy(df.features,metric = "gower")
all.complete <- NbClust(data=df.features,diss=dm, distance=NULL, min.nc=2, max.nc=10, method="complete", index="all")
all.single <- NbClust(data=df.features,diss=dm, distance=NULL, min.nc=2, max.nc=10, method="single", index="all")
all.avg <- NbClust(data=df.features,diss=dm, distance=NULL, min.nc=2, max.nc=10, method="average", index="all")
factoextra::fviz_nbclust(all.complete) + theme_minimal() + theme(text = element_text(size=20), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("")
factoextra::fviz_nbclust(all.complete) + theme_minimal() + theme(text = element_text(size=20), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("")
factoextra::fviz_nbclust(all.single) + theme_minimal() + theme(text = element_text(size=20), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("")
factoextra::fviz_nbclust(all.avg) + theme_minimal() + theme(text = element_text(size=20), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("")
factoextra::fviz_nbclust(all.complete) + theme_minimal() + theme(text = element_text(size=20), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("Optimal number of clusters for complete likage")
factoextra::fviz_nbclust(all.single) + theme_minimal() + theme(text = element_text(size=20), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("Optimal number of clusters for single likage")
factoextra::fviz_nbclust(all.avg) + theme_minimal() + theme(text = element_text(size=20), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("Optimal number of clusters for average likage")
library(factoextra)
dataTransform <- preProcess(train.balanced, method=c("center", "scale"))
train.balanced.std <- predict(dataTransform, train.balanced)
set.seed(123)
k.grid <- data.frame(k=1:25)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)
knn.model <- train(train.balanced.std[c(1,2)], as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
dm.mat <- as.matrix(dm)
agnes.avg      <- agnes(x=dm.mat, diss=TRUE, method="average")
agnes.single   <- agnes(x=dm.mat, diss=TRUE, method="single")
agnes.complete <- agnes(x=dm.mat, diss=TRUE, method="complete")
fviz_dend(agnes.avg, cex=0.4, k=2)+ theme(text = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("")
fviz_dend(agnes.avg, cex=0.4, k=2)+ theme(text = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("Agnes dendrogram for average linkage")
```{r}
fviz_dend(agnes.avg, cex=0.4, k=2)+ theme(text = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("Agnes dendrogram for average linkage")
fviz_dend(agnes.single, cex=0.4, k=2)+ theme(text = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("Agnes dendrogram for single linkage")
fviz_dend(agnes.avg, cex=0.4, k=2)+ theme(text = element_text(size=20), axis.text.y = element_text(size=20)) + ggtitle("Agnes dendrogram for complete linkage")
# Remove unnecessary features
df1[, categorical] <-  lapply(df1[, categorical], as.factor)
# df.mds <- subset(df1, col=-c("Class"))
df.mds <- df1[,2:20]
# Derive dissimilarities between objects
dissimilarities <- daisy(df.mds, stand = T)
dis.matrix <- as.matrix(dissimilarities)
d.max <- 6
scree.plot <- function(d, k) {
stresses <- sammon(d, k = k)$stress
for(i in rev(seq(k-1)))
stresses <- append(stresses, sammon(d, k = i)$stress)
plot(seq(k),rev(stresses), type="b", xaxp=c(1,k, k-1), ylab="Stress", xlab="Number of dimensions")
}
scree.plot(dissimilarities, k = d.max <- 6)
stress.vec <- numeric(d.max)
par(mfrow=c(3,2))
for (d in 1:d.max)
{
mds.k <- sammon(dis.matrix, k = d)
STRESS <- mds.k$stress
stress.vec[d] <- STRESS
# Shepard diagram
shep <- Shepard(dissimilarities, mds.k$points, p=d)
plot(shep, pch=".", main=paste0("Shepard diagram (d=",d,")"),
cex=0.5, xlab="original distance",  ylab="distance after MDS mapping")
lines(shep$x, shep$yf, type = "S", col="red", lty=2)
grid()
legend(x="topleft",legend=paste("STRESS = ",signif(STRESS,3)), bg="azure2")
}
set.seed(123)
df.mds <- sammon(dis.matrix, k = 2)$points
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
# text(mds.results[,1], mds.results[,2], car.names, col="brown",cex=.8)
set.seed(123)
inTrain <- createDataPartition(y=df.mds$Class, times=1, p=0.75, list=FALSE)
df.mds <- data.frame(df.mds)
df.mds$Class <- as.factor(df1$Class)
df.mds
set.seed(123)
inTrain <- createDataPartition(y=df.mds$Class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,]
test <- df.mds[-inTrain,]
n_new <- sum(train$Class == 0) - sum(train$Class == 1)
newMWMOTE <- mwmote(train, numInstances = n_new)
train.balanced <- rbind(train, newMWMOTE)
prop.table(table(train.balanced$Class))
plotComparison(train, train.balanced, attrs = c("X1", "X2"))
train.balanced$Class <- as.numeric(train.balanced$Class)
train.balanced$Class <- train.balanced$Class - 1
test.num <- test
test.num$Class <- as.numeric(test.num$Class)
test.num$Class <- test.num$Class - 1
# some useful functions
slope <- function(model){
-model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
-(model$coefficients[1]-0.5)/model$coefficients[3]
}
lr_pred <- function(model,test,thr){
pred <- predict(model, test)
pred[pred > thr] = 1
pred[pred < thr] = 0
return(pred)
}
lda_pred <- function(model,test,thr){
pred <- predict(model,test)$posterior[,2]
pred[pred > thr] = 1
pred[pred < thr] = 0
return(pred)
}
confusion.matrix.results <- function(cm){
byClass <- cm$byClass[c(1, 2, 7, 11)]
overall <- cm$overall[1:2]
statistics <- append(overall, byClass)
return(statistics)
}
metrices <- function(pred, real){
confusion.matrix <- confusionMatrix(table(pred, real), positive = "1")
statistics <- confusion.matrix.results(confusion.matrix)
p <- precision(table(pred, real), relevant = "1")
return(c(statistics,p))
}
model.lm <- lm(Class~X1+X2, data=train.balanced)
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
metrices(lr_pred(model.lm, test.num, 0.5), test.num$Class)
n <- nrow(test.num)
model.logit <-  glm(Class~X1+X2, data=train.balanced, family=binomial(link="logit"))
pred.prob <- predict(model.logit, test.num, type = "response")
pred.prob.plot <- data.frame(x = 1:n, probability = pred.prob, classes = test$Class)
ggplot(data = pred.prob.plot, aes(x = x, y = probability)) +
geom_point(aes(color = test$Class))
prob.to.labels <- function(probs, cutoff){
classes <- rep("0",length(probs))
classes[probs>cutoff] <- "1"
return(as.factor(classes))
}
pred.labels <- prob.to.labels(probs=pred.prob, cutoff=0.5)
real.labels <- test.num$Class
cm <- confusionMatrix(table(pred.labels, real.labels), positive = "1")
logit <- confusion.matrix.results(cm)
logit
dataTransform <- preProcess(train.balanced, method=c("center", "scale"))
train.balanced.std <- predict(dataTransform, train.balanced)
set.seed(123)
k.grid <- data.frame(k=1:25)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)
knn.model <- train(train.balanced.std[c(1,2)], as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
factoextra::fviz_nbclust(all.complete) + theme_minimal() + ggtitle("Optimal number of clusters for complete likage")
factoextra::fviz_nbclust(all.single) + theme_minimal() +ggtitle("Optimal number of clusters for single likage")
factoextra::fviz_nbclust(all.avg) + theme_minimal() + ggtitle("Optimal number of clusters for average likage")
fviz_dend(agnes.avg, cex=0.4, k=2) + ggtitle("Agnes dendrogram for average linkage")
#+ theme(text = element_text(size=20), axis.text.y = element_text(size=20))
fviz_dend(agnes.single, cex=0.4, k=2)+ ggtitle("Agnes dendrogram for single linkage")
fviz_dend(agnes.avg, cex=0.4, k=2) + ggtitle("Agnes dendrogram for complete linkage")
(agnes.avg.k2 <- cutree(agnes.avg, k=2))  # 2 clusters
(agnes.avg.k3 <- cutree(agnes.avg, k=3))  # 3 clusters
(agnes.avg.k4 <- cutree(agnes.avg, k=4))  # 4 clusters
# compare cluster sizes
table(agnes.avg.k2)
agnes.avg.k2
df1$Class
table(agnes.avg.k3)
table(agnes.avg.k4)
(agnes.avg.k2 <- cutree(agnes.avg, k=2))  # 2 clusters
(agnes.avg.k3 <- cutree(agnes.avg, k=3))  # 3 clusters
(agnes.avg.k4 <- cutree(agnes.avg, k=4))  # 4 clusters
# compare cluster sizes
table(agnes.avg.k2)
table(agnes.avg.k3)
table(agnes.avg.k4)
newMWMOTE <- mwmote(df.features, numInstances = n_new)
df.features
n_new <- sum(df1$Class == 0) - sum(df1$Class == 1)
newMWMOTE <- mwmote(df.features, numInstances = n_new)
n_new
View(df)
