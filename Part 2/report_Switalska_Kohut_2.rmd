---
title: "Hepatisis data analysis"
output:
  pdf_document: 
    number_sections: yes
  html_document:
    number_sections: yes
---

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(dplyr)
library(caret)
library(MASS)
library(cluster)
library(imbalance)
library(NbClust)
library(factoextra)
source("fviz_nbclust_fixed.R") # fixes the bug in factoextra::fviz_nbclust(...)
```

# Introduction

As we mentioned before, we have a lot of missing values, and we should impute them. We found that the knn imputing method gave better results during the last part, so we will use it now. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
df <- read.table("hepatitis.data", sep = ",")
colnames(df) <- c("Class","Age","Sex","Steroid","Antivirals","Fatigue","Malaise","Anorexia","LiverBig","LiverFirm","SpleenPalpable","Spiders","Ascites","Varices","Bilirubin","AlkPhosphate","Sgot","Albumin","Protime","Histology") # nolint

df[df == "?"] <- NA

df <- mutate_all(df, function(x) as.numeric(as.character(x)))
categorical <- c(1, 3:14, 20)
df[, categorical] <- replace(df[, categorical], df[, categorical] == 2, 0)
df[, categorical] <-  lapply(df[, categorical], as.factor)

set.seed(123)
df.new <- mutate_all(df, function(x) as.numeric(as.character(x)))
data_transform <- preProcess(df.new, method = "knnImpute")
df1 <- predict(data_transform, df.new)

unstandarize <- function(data){
  for (i in 1:20) {
    column <- df.new[, i]
    if (i %in% c(15, 18)) {
      data[, i] <- data[, i] * sd(na.omit(column)) + mean(na.omit(column))
    } else {
      data[, i] <- round(data[, i] * sd(na.omit(column)) + mean(na.omit(column)))
    }
  }
  return(data)
}

df1.standarized <- df1
df1 <- unstandarize(df1)
```

# Cluster analysis 

## Number of clusters
```{r}
dm <- daisy(df.features,metric = "gower")
dm.mat <- as.matrix(dm)

all.complete <- NbClust(data=df.features,diss=dm, distance=NULL, min.nc=2, max.nc=10, method="complete", index="all")
all.single <- NbClust(data=df.features,diss=dm, distance=NULL, min.nc=2, max.nc=10, method="single", index="all")
all.avg <- NbClust(data=df.features,diss=dm, distance=NULL, min.nc=2, max.nc=10, method="average", index="all")
```

```{r, echo=FALSE}
factoextra::fviz_nbclust(all.complete) + theme_minimal() + ggtitle("Optimal number of clusters for complete likage")
```

```{r, echo=FALSE}
factoextra::fviz_nbclust(all.single) + theme_minimal() +ggtitle("Optimal number of clusters for single likage")
```

```{r, echo=FALSE}
factoextra::fviz_nbclust(all.avg) + theme_minimal() + ggtitle("Optimal number of clusters for average likage")
```


## K-means (M)

## PAM (M)

## AGNES (I)
We present AGNES clustering for all features and different linkage methods on the figures below. It can be seen that the single linkage method performs poorly by assigning almost all observations to one cluster.
```{r}
agnes.avg      <- agnes(x=dm.mat, diss=TRUE, method="average")
agnes.single   <- agnes(x=dm.mat, diss=TRUE, method="single")
agnes.complete <- agnes(x=dm.mat, diss=TRUE, method="complete")
```

```{r}
fviz_dend(agnes.avg, cex=0.4, k=2) + ggtitle("Agnes dendrogram for average linkage")
#+ theme(text = element_text(size=20), axis.text.y = element_text(size=20))
```
```{r}
fviz_dend(agnes.single, cex=0.4, k=2)+ ggtitle("Agnes dendrogram for single linkage")
```

```{r}
fviz_dend(agnes.avg, cex=0.4, k=2) + ggtitle("Agnes dendrogram for complete linkage")
```
```{r}
(agnes.avg.k2 <- cutree(agnes.avg, k=2))  # 2 clusters
(agnes.avg.k3 <- cutree(agnes.avg, k=3))  # 3 clusters
(agnes.avg.k4 <- cutree(agnes.avg, k=4))  # 4 clusters

# compare cluster sizes
table(agnes.avg.k2)
table(agnes.avg.k3)
table(agnes.avg.k4)
```

## DIANA (I)

## Fuzzy C-means (M)

## DBSCAN (I)


#	Dimension reduction method

As we have both numerical and categorical attributes, we can not use PCA (Principal Component Analysis) method. We decided to use MDS (MultiDimensional Scaling). 

## MDS (M)
We use standardized data. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, results = FALSE}
# Remove unnecessary features
df1[, categorical] <-  lapply(df1[, categorical], as.factor)
# df.mds <- subset(df1, col=-c("Class"))
df.mds <- df1[,2:20]

# Derive dissimilarities between objects
dissimilarities <- daisy(df.mds, stand = T)
dis.matrix <- as.matrix(dissimilarities)
```

Let us look at the scree plot.
```{r, echo=FALSE, results=FALSE}
d.max <- 6

scree.plot <- function(d, k) {
    stresses <- sammon(d, k = k)$stress
    for(i in rev(seq(k-1)))  
        stresses <- append(stresses, sammon(d, k = i)$stress)
    plot(seq(k),rev(stresses), type="b", xaxp=c(1,k, k-1), ylab="Stress", xlab="Number of dimensions")
}

scree.plot(dissimilarities, k = d.max <- 6)
```

The scree plot shows a clear elbow at dimension = 2, which suggests that a 2D solution should be adequate. Now we check out the Shepard diagram:

```{r, echo=FALSE, results=FALSE}
stress.vec <- numeric(d.max)

par(mfrow=c(3,2))

for (d in 1:d.max)
{
  mds.k <- sammon(dis.matrix, k = d)
  STRESS <- mds.k$stress

  stress.vec[d] <- STRESS
  
  # Shepard diagram
  shep <- Shepard(dissimilarities, mds.k$points, p=d)
  plot(shep, pch=".", main=paste0("Shepard diagram (d=",d,")"),
       cex=0.5, xlab="original distance",  ylab="distance after MDS mapping")
  lines(shep$x, shep$yf, type = "S", col="red", lty=2)
  grid()
  legend(x="topleft",legend=paste("STRESS = ",signif(STRESS,3)), bg="azure2")
}
```

The plot for d = 2 shows not so big amount of spread around the fitted function, which also indicates a good fit of the 2D solution.

```{r, echo=FALSE, results=FALSE}
set.seed(123) 
df.mds <- sammon(dis.matrix, k = 2)$points
```

```{r, echo=FALSE}
plot(df.mds[,1], df.mds[,2], col = factor(df1$Class), pch=16)
# text(mds.results[,1], mds.results[,2], car.names, col="brown",cex=.8)
```

So, we will use MDS for 2 dimensions. The classes are separated but also overlap, so in the future we may have a problem with classification.

```{r, echo=FALSE, results=FALSE}
df.mds <- data.frame(df.mds)
df.mds$Class <- as.factor(df1$Class)
df.mds
```

## Classification (I)

```{r, echo=FALSE}
set.seed(123) 
inTrain <- createDataPartition(y=df.mds$Class, times=1, p=0.75, list=FALSE)
train <- df.mds[inTrain,]
test <- df.mds[-inTrain,]

n_new <- sum(train$Class == 0) - sum(train$Class == 1)

newMWMOTE <- mwmote(train, numInstances = n_new)
train.balanced <- rbind(train, newMWMOTE)
prop.table(table(train.balanced$Class))
```
Below we can check the comparison of values before and after oversampling for three selected variables.
```{r}
plotComparison(train, train.balanced, attrs = c("X1", "X2"))
```
### Linear regression
```{r, echo=FALSE}
train.balanced$Class <- as.numeric(train.balanced$Class)
train.balanced$Class <- train.balanced$Class - 1

test.num <- test
test.num$Class <- as.numeric(test.num$Class)
test.num$Class <- test.num$Class - 1

# some useful functions
slope <- function(model){
  -model$coefficients[2]/model$coefficients[3]
}
intercept <- function(model){
  -(model$coefficients[1]-0.5)/model$coefficients[3]
}
lr_pred <- function(model,test,thr){
  pred <- predict(model, test)
  pred[pred > thr] = 1
  pred[pred < thr] = 0
  return(pred)
}
lda_pred <- function(model,test,thr){
  pred <- predict(model,test)$posterior[,2]
  pred[pred > thr] = 1
  pred[pred < thr] = 0
  return(pred)
}
confusion.matrix.results <- function(cm){
  byClass <- cm$byClass[c(1, 2, 7, 11)]
  overall <- cm$overall[1:2]
  statistics <- append(overall, byClass)
  return(statistics)
}
metrices <- function(pred, real){
  confusion.matrix <- confusionMatrix(table(pred, real), positive = "1")
  statistics <- confusion.matrix.results(confusion.matrix)
  p <- precision(table(pred, real), relevant = "1")
  return(c(statistics,p))
}
```

```{r,echo=FALSE}
model.lm <- lm(Class~X1+X2, data=train.balanced) 
ggplot(test, aes(x=X1, y=X2,color=Class)) + geom_point(size=1) + geom_abline(slope=slope(model.lm),intercept=intercept(model.lm))
```
```{r}
metrices(lr_pred(model.lm, test.num, 0.5), test.num$Class)
```
### Logistic regression
```{r}
n <- nrow(test.num)
model.logit <-  glm(Class~X1+X2, data=train.balanced, family=binomial(link="logit"))
pred.prob <- predict(model.logit, test.num, type = "response")
pred.prob.plot <- data.frame(x = 1:n, probability = pred.prob, classes = test$Class)
ggplot(data = pred.prob.plot, aes(x = x, y = probability)) +
    geom_point(aes(color = test$Class))
```
```{r}
prob.to.labels <- function(probs, cutoff){
  classes <- rep("0",length(probs))
  classes[probs>cutoff] <- "1"
  return(as.factor(classes))
}

pred.labels <- prob.to.labels(probs=pred.prob, cutoff=0.5)
real.labels <- test.num$Class
cm <- confusionMatrix(table(pred.labels, real.labels), positive = "1")
logit <- confusion.matrix.results(cm)
logit
```
### KNN
```{r}
dataTransform <- preProcess(train.balanced, method=c("center", "scale"))
train.balanced.std <- predict(dataTransform, train.balanced)

set.seed(123) 
k.grid <- data.frame(k=1:25)
cvControl <- trainControl(method="repeatedcv",number=5, repeats=5)
knn.model <- train(train.balanced.std[c(1,2)], as.factor(train.balanced$Class), method="knn", tuneGrid =  k.grid, trControl=cvControl)
ggplot(knn.model)+ geom_vline(xintercept = knn.model$bestTune[[1]], col="red", lty=2)
```

## Clustering (M)

#	Conclusions (I)

#	Further research suggestions (I)

